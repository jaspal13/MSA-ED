{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking species wise class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "speciesList=[\"Human\",\"Cow\",\"Cat\",\"Mouse\",\"Pig\",\"Rhesus\",\"Microbat\",\"Elephant\",\"Rabbit\",\"Shrew\"]\n",
    "#a = [1,1,1,1,2,2,2,2,3,3,4,5,5]\n",
    "def get_species_class_distribution(speciesName,inputList):\n",
    "    indexOfSpecies=speciesList.index(speciesName)\n",
    "    tempList=[]\n",
    "    for i in inputList:\n",
    "        tempList.append(i[indexOfSpecies])\n",
    "    assert(len(tempList)==len(inputList))\n",
    "    counter=collections.Counter(tempList)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_species_wise_class_dist(LabelList):\n",
    "    \n",
    "    stringTable=\"<table><tr><th>Species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th></tr>\"\n",
    "    for i in speciesList[1:]:\n",
    "        counter=get_species_class_distribution(i,LabelList)\n",
    "        stringTable+=\"<tr><td>\"+i+\"</td><td>\"+str(round(counter[0]*100.0/len(LabelList),3))+\"</td><td>\"+str(round(counter[1]*100.0/len(LabelList),3))+\"</td><td>\"+str(round(counter[2]*100.0/len(LabelList),3))+\"</td><td>\"+str(round(counter[3]*100.0/len(LabelList),3))+\"</td><td>\"+str(round(counter[4]*100.0/len(LabelList),3))+\"</td><td>\"\n",
    "    stringTable+=\"</table>\"\n",
    "    return stringTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cells load data from 2 files\n",
    "1. The training indices files which contains the indices of each species\n",
    "2. The labels file which contain the output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"TrainingIndices10Species_Updated_withAncestors_ModifiedDic.json\", 'r') as f:\n",
    "    trainingIndices = json.load(f)\n",
    "with open(\"TrainingLabel10Species_Updated_WithAncestors_ModifiedDic.json\", 'r') as f:\n",
    "    trainingLabels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Done\n",
      "[u'3634915_Human.chr22_1', u'4133722_Cow.chr22.1_1', u'4189836_Cat.chr22.1_-1', u'-19', u'-19', u'3292135_Rhesus.chr22_-1', u'1889148_Microbat.chr22.2.1.1_1', u'2096506_Elephant.chr22_-1', u'2702055_Rabbit.chr22.2.1.1_1', u'-19']\n",
      "[-1, 0, 0, 3, 3, 0, 0, 4, 0, 3]\n",
      "10471799\n",
      "10471799\n"
     ]
    }
   ],
   "source": [
    "print \"Data Load Done\"\n",
    "print trainingIndices[0]\n",
    "print trainingLabels[0]\n",
    "print len(trainingIndices)\n",
    "print len(trainingLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the human index in the label list as well so that we can map corresponding list in the indices with that of the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for enum,i in enumerate(trainingLabels):\n",
    "    indexStr=str(trainingIndices[enum][0])\n",
    "    firstUnderscore=indexStr.index('_')\n",
    "    intIndex=int(indexStr[0:firstUnderscore])\n",
    "    i[0]=intIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'3634915_Human.chr22_1', u'4133722_Cow.chr22.1_1', u'4189836_Cat.chr22.1_-1', u'-19', u'-19', u'3292135_Rhesus.chr22_-1', u'1889148_Microbat.chr22.2.1.1_1', u'2096506_Elephant.chr22_-1', u'2702055_Rabbit.chr22.2.1.1_1', u'-19']\n",
      "[3634915, 0, 0, 3, 3, 0, 0, 4, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "print trainingIndices[0]\n",
    "print trainingLabels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can map X to y. We can desample the data to have equal classes for Cow. First lets check the class distribution before desampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentDistribution = get_species_wise_class_dist(trainingLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table><tr><th>Species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th></tr><tr><td>Cow</td><td>40.076</td><td>19.257</td><td>9.142</td><td>23.82</td><td>7.706</td><td><tr><td>Cat</td><td>53.187</td><td>14.717</td><td>8.718</td><td>15.56</td><td>7.819</td><td><tr><td>Mouse</td><td>26.723</td><td>24.444</td><td>8.155</td><td>34.087</td><td>6.592</td><td><tr><td>Pig</td><td>50.141</td><td>15.821</td><td>9.012</td><td>17.561</td><td>7.464</td><td><tr><td>Rhesus</td><td>78.394</td><td>5.488</td><td>5.678</td><td>6.136</td><td>4.303</td><td><tr><td>Microbat</td><td>49.576</td><td>16.676</td><td>8.914</td><td>17.777</td><td>7.056</td><td><tr><td>Elephant</td><td>51.389</td><td>15.801</td><td>8.707</td><td>16.754</td><td>7.349</td><td><tr><td>Rabbit</td><td>50.267</td><td>15.603</td><td>9.037</td><td>16.402</td><td>8.69</td><td><tr><td>Shrew</td><td>24.88</td><td>24.961</td><td>8.156</td><td>35.899</td><td>6.105</td><td></table>\n"
     ]
    }
   ],
   "source": [
    "print currentDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><th>Species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th></tr><tr><td>Cow</td><td>40.076</td><td>19.257</td><td>9.142</td><td>23.82</td><td>7.706</td><td><tr><td>Cat</td><td>53.187</td><td>14.717</td><td>8.718</td><td>15.56</td><td>7.819</td><td><tr><td>Mouse</td><td>26.723</td><td>24.444</td><td>8.155</td><td>34.087</td><td>6.592</td><td><tr><td>Pig</td><td>50.141</td><td>15.821</td><td>9.012</td><td>17.561</td><td>7.464</td><td><tr><td>Rhesus</td><td>78.394</td><td>5.488</td><td>5.678</td><td>6.136</td><td>4.303</td><td><tr><td>Microbat</td><td>49.576</td><td>16.676</td><td>8.914</td><td>17.777</td><td>7.056</td><td><tr><td>Elephant</td><td>51.389</td><td>15.801</td><td>8.707</td><td>16.754</td><td>7.349</td><td><tr><td>Rabbit</td><td>50.267</td><td>15.603</td><td>9.037</td><td>16.402</td><td>8.69</td><td><tr><td>Shrew</td><td>24.88</td><td>24.961</td><td>8.156</td><td>35.899</td><td>6.105</td><td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can desample the label list to contain equal class distribution of cow species. First let's calculate the total number of records in the minimum class,i.e. class 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806956.83094\n"
     ]
    }
   ],
   "source": [
    "minNumberOfRecords = (7.706/100)*len(trainingLabels)\n",
    "print minNumberOfRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4034749\n"
     ]
    }
   ],
   "source": [
    "DesampledLabelList =[]\n",
    "count0,count1,count2,count3,count4 =0,0,0,0,0\n",
    "for ind,i in enumerate(trainingLabels):\n",
    "    if(i[1] == 2):\n",
    "        count2+=1\n",
    "        if(count2<minNumberOfRecords):\n",
    "            DesampledLabelList.append(i)\n",
    "    if(i[1] == 0):\n",
    "        count0+=1\n",
    "        if(count0<minNumberOfRecords):\n",
    "            DesampledLabelList.append(i)\n",
    "    if(i[1] == 1):\n",
    "        count1+=1\n",
    "        if(count1<minNumberOfRecords):\n",
    "            DesampledLabelList.append(i)\n",
    "    if(i[1] == 3):\n",
    "        count3+=1\n",
    "        if(count3<minNumberOfRecords):\n",
    "            DesampledLabelList.append(i)\n",
    "    if(i[1] == 4):\n",
    "        count4+=1\n",
    "        if(count4<minNumberOfRecords):\n",
    "            DesampledLabelList.append(i)\n",
    "print len(DesampledLabelList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDistribution = get_species_wise_class_dist(DesampledLabelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table><tr><th>Species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th></tr><tr><td>Cow</td><td>20.0</td><td>20.0</td><td>20.0</td><td>20.0</td><td>19.999</td><td><tr><td>Cat</td><td>44.846</td><td>16.363</td><td>14.545</td><td>15.065</td><td>9.18</td><td><tr><td>Mouse</td><td>22.405</td><td>25.695</td><td>12.717</td><td>32.09</td><td>7.093</td><td><tr><td>Pig</td><td>40.571</td><td>17.637</td><td>15.313</td><td>16.919</td><td>9.56</td><td><tr><td>Rhesus</td><td>71.657</td><td>7.303</td><td>9.322</td><td>6.235</td><td>5.483</td><td><tr><td>Microbat</td><td>41.63</td><td>18.172</td><td>14.675</td><td>17.157</td><td>8.366</td><td><tr><td>Elephant</td><td>44.187</td><td>17.703</td><td>14.079</td><td>16.063</td><td>7.969</td><td><tr><td>Rabbit</td><td>44.098</td><td>16.467</td><td>14.038</td><td>15.857</td><td>9.541</td><td><tr><td>Shrew</td><td>20.142</td><td>26.811</td><td>13.064</td><td>33.566</td><td>6.416</td><td></table>\n",
      "[3634915, 0, 0, 3, 3, 0, 0, 4, 0, 3]\n",
      "[u'3634915_Human.chr22_1', u'4133722_Cow.chr22.1_1', u'4189836_Cat.chr22.1_-1', u'-19', u'-19', u'3292135_Rhesus.chr22_-1', u'1889148_Microbat.chr22.2.1.1_1', u'2096506_Elephant.chr22_-1', u'2702055_Rabbit.chr22.2.1.1_1', u'-19']\n"
     ]
    }
   ],
   "source": [
    "print newDistribution\n",
    "print DesampledLabelList[0]\n",
    "print trainingIndices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><th>Species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th></tr><tr><td>Cow</td><td>20.0</td><td>20.0</td><td>20.0</td><td>20.0</td><td>19.999</td><td><tr><td>Cat</td><td>44.846</td><td>16.363</td><td>14.545</td><td>15.065</td><td>9.18</td><td><tr><td>Mouse</td><td>22.405</td><td>25.695</td><td>12.717</td><td>32.09</td><td>7.093</td><td><tr><td>Pig</td><td>40.571</td><td>17.637</td><td>15.313</td><td>16.919</td><td>9.56</td><td><tr><td>Rhesus</td><td>71.657</td><td>7.303</td><td>9.322</td><td>6.235</td><td>5.483</td><td><tr><td>Microbat</td><td>41.63</td><td>18.172</td><td>14.675</td><td>17.157</td><td>8.366</td><td><tr><td>Elephant</td><td>44.187</td><td>17.703</td><td>14.079</td><td>16.063</td><td>7.969</td><td><tr><td>Rabbit</td><td>44.098</td><td>16.467</td><td>14.038</td><td>15.857</td><td>9.541</td><td><tr><td>Shrew</td><td>20.142</td><td>26.811</td><td>13.064</td><td>33.566</td><td>6.416</td><td></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del trainingLabels# this list is no longer required we have the desampled list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fetch the nucleotides for the indices in trainingIndices list but we need to keep the human index intact even after getting the nucleotides, so lets add an additional human index to the trainingIndices list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for enum,i in enumerate(trainingIndices):\n",
    "    indexStr=str(i[0])\n",
    "    firstUnderscore=indexStr.index('_')\n",
    "    intIndex=int(indexStr[0:firstUnderscore])\n",
    "    i.append(intIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'3634915_Human.chr22_1', u'4133722_Cow.chr22.1_1', u'4189836_Cat.chr22.1_-1', u'-19', u'-19', u'3292135_Rhesus.chr22_-1', u'1889148_Microbat.chr22.2.1.1_1', u'2096506_Elephant.chr22_-1', u'2702055_Rabbit.chr22.2.1.1_1', u'-19', 3634915]\n",
      "[u'3634915_Human.chr22_1', u'4133722_Cow.chr22.1_1', u'4189836_Cat.chr22.1_-1', u'-19', u'-19', u'3292135_Rhesus.chr22_-1', u'1889148_Microbat.chr22.2.1.1_1', u'2096506_Elephant.chr22_-1', u'2702055_Rabbit.chr22.2.1.1_1', u'-19']\n"
     ]
    }
   ],
   "source": [
    "print trainingIndices[0]\n",
    "print trainingIndices[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting nucleotides from indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the trainingIndices list; we can proceed to get the nucleotides from the fasta files of those species and replace the indices with the appropriate nucleotides. First we will load all the fastas for the species in their specific lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "humanSeqName=[]\n",
    "humanSeqFasta=[]\n",
    "cowSeqName=[]\n",
    "cowSeqFasta=[]\n",
    "mouseSeqName=[]\n",
    "mouseSeqFasta=[]\n",
    "microbatSeqName=[]\n",
    "microbatSeqFasta=[]\n",
    "pigSeqName=[]\n",
    "pigSeqFasta=[]\n",
    "rhesusSeqName=[]\n",
    "rhesusSeqFasta=[]\n",
    "rabbitSeqName=[]\n",
    "rabbitSeqFasta=[]\n",
    "shrewSeqName=[]\n",
    "shrewSeqFasta=[]\n",
    "elephantSeqName=[]\n",
    "elephantSeqFasta=[]\n",
    "catSeqName=[]\n",
    "catSeqFasta=[]\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Human\", \"fasta\"):\n",
    "        humanSeqName.append(seq_record.id)\n",
    "        humanSeqFasta.append(str(seq_record.seq))\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Cow\", \"fasta\"):\n",
    "        cowSeqName.append(seq_record.id)\n",
    "        cowSeqFasta.append(seq_record.seq)\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Mouse\", \"fasta\"):\n",
    "        mouseSeqName.append(seq_record.id)\n",
    "        mouseSeqFasta.append(seq_record.seq)\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Microbat\", \"fasta\"):\n",
    "        microbatSeqName.append(seq_record.id)\n",
    "        microbatSeqFasta.append(seq_record.seq)\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Pig\", \"fasta\"):\n",
    "        pigSeqName.append(seq_record.id)\n",
    "        pigSeqFasta.append(seq_record.seq)\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Rhesus\", \"fasta\"):\n",
    "        rhesusSeqName.append(seq_record.id)\n",
    "        rhesusSeqFasta.append(seq_record.seq)\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Rabbit\", \"fasta\"):\n",
    "        rabbitSeqName.append(seq_record.id)\n",
    "        rabbitSeqFasta.append(seq_record.seq)\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Shrew\", \"fasta\"):\n",
    "        shrewSeqName.append(seq_record.id)\n",
    "        shrewSeqFasta.append(seq_record.seq)\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Elephant\", \"fasta\"):\n",
    "        elephantSeqName.append(seq_record.id)\n",
    "        elephantSeqFasta.append(seq_record.seq)\n",
    "for seq_record in SeqIO.parse(\"importantFiles/Cat\", \"fasta\"):\n",
    "        catSeqName.append(seq_record.id)\n",
    "        catSeqFasta.append(seq_record.seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Case of negative strand we need the complementary base pairs. Creating the function which returns the complementary base pair of a nucleotide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getComplementBasePair(inputBasePair):\n",
    "    if inputBasePair=='A':\n",
    "        return 'T'\n",
    "    elif inputBasePair=='T':\n",
    "        return 'A'\n",
    "    elif inputBasePair=='C':\n",
    "        return 'G'\n",
    "    elif inputBasePair=='G':\n",
    "        return 'C'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell processes the combinedList and using the nucleotide list of each species replaces the combined list with nucleotides. At the end of the cell, you have a combined list with nucleotides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length: 10471799\n",
      "0  done\n",
      "10000  done\n",
      "20000  done\n",
      "30000  done\n",
      "40000  done\n",
      "50000  done\n",
      "60000  done\n",
      "70000  done\n",
      "80000  done\n",
      "90000  done\n",
      "100000  done\n",
      "110000  done\n",
      "120000  done\n",
      "130000  done\n",
      "140000  done\n",
      "150000  done\n",
      "160000  done\n",
      "170000  done\n",
      "180000  done\n",
      "190000  done\n",
      "200000  done\n",
      "210000  done\n",
      "220000  done\n",
      "230000  done\n",
      "240000  done\n",
      "250000  done\n",
      "260000  done\n",
      "270000  done\n",
      "280000  done\n",
      "290000  done\n",
      "300000  done\n",
      "310000  done\n",
      "320000  done\n",
      "330000  done\n",
      "340000  done\n",
      "350000  done\n",
      "360000  done\n",
      "370000  done\n",
      "380000  done\n",
      "390000  done\n",
      "400000  done\n",
      "410000  done\n",
      "420000  done\n",
      "430000  done\n",
      "440000  done\n",
      "450000  done\n",
      "460000  done\n",
      "470000  done\n",
      "480000  done\n",
      "490000  done\n",
      "500000  done\n",
      "510000  done\n",
      "520000  done\n",
      "530000  done\n",
      "540000  done\n",
      "550000  done\n",
      "560000  done\n",
      "570000  done\n",
      "580000  done\n",
      "590000  done\n",
      "600000  done\n",
      "610000  done\n",
      "620000  done\n",
      "630000  done\n",
      "640000  done\n",
      "650000  done\n",
      "660000  done\n",
      "670000  done\n",
      "680000  done\n",
      "690000  done\n",
      "700000  done\n",
      "710000  done\n",
      "720000  done\n",
      "730000  done\n",
      "740000  done\n",
      "750000  done\n",
      "760000  done\n",
      "770000  done\n",
      "780000  done\n",
      "790000  done\n",
      "800000  done\n",
      "810000  done\n",
      "820000  done\n",
      "830000  done\n",
      "840000  done\n",
      "850000  done\n",
      "860000  done\n",
      "870000  done\n",
      "880000  done\n",
      "890000  done\n",
      "900000  done\n",
      "910000  done\n",
      "920000  done\n",
      "930000  done\n",
      "940000  done\n",
      "950000  done\n",
      "960000  done\n",
      "970000  done\n",
      "980000  done\n",
      "990000  done\n",
      "1000000  done\n",
      "1010000  done\n",
      "1020000  done\n",
      "1030000  done\n",
      "1040000  done\n",
      "1050000  done\n",
      "1060000  done\n",
      "1070000  done\n",
      "1080000  done\n",
      "1090000  done\n",
      "1100000  done\n",
      "1110000  done\n",
      "1120000  done\n",
      "1130000  done\n",
      "1140000  done\n",
      "1150000  done\n",
      "1160000  done\n",
      "1170000  done\n",
      "1180000  done\n",
      "1190000  done\n",
      "1200000  done\n",
      "1210000  done\n",
      "1220000  done\n",
      "1230000  done\n",
      "1240000  done\n",
      "1250000  done\n",
      "1260000  done\n",
      "1270000  done\n",
      "1280000  done\n",
      "1290000  done\n",
      "1300000  done\n",
      "1310000  done\n",
      "1320000  done\n",
      "1330000  done\n",
      "1340000  done\n",
      "1350000  done\n",
      "1360000  done\n",
      "1370000  done\n",
      "1380000  done\n",
      "1390000  done\n",
      "1400000  done\n",
      "1410000  done\n",
      "1420000  done\n",
      "1430000  done\n",
      "1440000  done\n",
      "1450000  done\n",
      "1460000  done\n",
      "1470000  done\n",
      "1480000  done\n",
      "1490000  done\n",
      "1500000  done\n",
      "1510000  done\n",
      "1520000  done\n",
      "1530000  done\n",
      "1540000  done\n",
      "1550000  done\n",
      "1560000  done\n",
      "1570000  done\n",
      "1580000  done\n",
      "1590000  done\n",
      "1600000  done\n",
      "1610000  done\n",
      "1620000  done\n",
      "1630000  done\n",
      "1640000  done\n",
      "1650000  done\n",
      "1660000  done\n",
      "1670000  done\n",
      "1680000  done\n",
      "1690000  done\n",
      "1700000  done\n",
      "1710000  done\n",
      "1720000  done\n",
      "1730000  done\n",
      "1740000  done\n",
      "1750000  done\n",
      "1760000  done\n",
      "1770000  done\n",
      "1780000  done\n",
      "1790000  done\n",
      "1800000  done\n",
      "1810000  done\n",
      "1820000  done\n",
      "1830000  done\n",
      "1840000  done\n",
      "1850000  done\n",
      "1860000  done\n",
      "1870000  done\n",
      "1880000  done\n",
      "1890000  done\n",
      "1900000  done\n",
      "1910000  done\n",
      "1920000  done\n",
      "1930000  done\n",
      "1940000  done\n",
      "1950000  done\n",
      "1960000  done\n",
      "1970000  done\n",
      "1980000  done\n",
      "1990000  done\n",
      "2000000  done\n",
      "2010000  done\n",
      "2020000  done\n",
      "2030000  done\n",
      "2040000  done\n",
      "2050000  done\n",
      "2060000  done\n",
      "2070000  done\n",
      "2080000  done\n",
      "2090000  done\n",
      "2100000  done\n",
      "2110000  done\n",
      "2120000  done\n",
      "2130000  done\n",
      "2140000  done\n",
      "2150000  done\n",
      "2160000  done\n",
      "2170000  done\n",
      "2180000  done\n",
      "2190000  done\n",
      "2200000  done\n",
      "2210000  done\n",
      "2220000  done\n",
      "2230000  done\n",
      "2240000  done\n",
      "2250000  done\n",
      "2260000  done\n",
      "2270000  done\n",
      "2280000  done\n",
      "2290000  done\n",
      "2300000  done\n",
      "2310000  done\n",
      "2320000  done\n",
      "2330000  done\n",
      "2340000  done\n",
      "2350000  done\n",
      "2360000  done\n",
      "2370000  done\n",
      "2380000  done\n",
      "2390000  done\n",
      "2400000  done\n",
      "2410000  done\n",
      "2420000  done\n",
      "2430000  done\n",
      "2440000  done\n",
      "2450000  done\n",
      "2460000  done\n",
      "2470000  done\n",
      "2480000  done\n",
      "2490000  done\n",
      "2500000  done\n",
      "2510000  done\n",
      "2520000  done\n",
      "2530000  done\n",
      "2540000  done\n",
      "2550000  done\n",
      "2560000  done\n",
      "2570000  done\n",
      "2580000  done\n",
      "2590000  done\n",
      "2600000  done\n",
      "2610000  done\n",
      "2620000  done\n",
      "2630000  done\n",
      "2640000  done\n",
      "2650000  done\n",
      "2660000  done\n",
      "2670000  done\n",
      "2680000  done\n",
      "2690000  done\n",
      "2700000  done\n",
      "2710000  done\n",
      "2720000  done\n",
      "2730000  done\n",
      "2740000  done\n",
      "2750000  done\n",
      "2760000  done\n",
      "2770000  done\n",
      "2780000  done\n",
      "2790000  done\n",
      "2800000  done\n",
      "2810000  done\n",
      "2820000  done\n",
      "2830000  done\n",
      "2840000  done\n",
      "2850000  done\n",
      "2860000  done\n",
      "2870000  done\n",
      "2880000  done\n",
      "2890000  done\n",
      "2900000  done\n",
      "2910000  done\n",
      "2920000  done\n",
      "2930000  done\n",
      "2940000  done\n",
      "2950000  done\n",
      "2960000  done\n",
      "2970000  done\n",
      "2980000  done\n",
      "2990000  done\n",
      "3000000  done\n",
      "3010000  done\n",
      "3020000  done\n",
      "3030000  done\n",
      "3040000  done\n",
      "3050000  done\n",
      "3060000  done\n",
      "3070000  done\n",
      "3080000  done\n",
      "3090000  done\n",
      "3100000  done\n",
      "3110000  done\n",
      "3120000  done\n",
      "3130000  done\n",
      "3140000  done\n",
      "3150000  done\n",
      "3160000  done\n",
      "3170000  done\n",
      "3180000  done\n",
      "3190000  done\n",
      "3200000  done\n",
      "3210000  done\n",
      "3220000  done\n",
      "3230000  done\n",
      "3240000  done\n",
      "3250000  done\n",
      "3260000  done\n",
      "3270000  done\n",
      "3280000  done\n",
      "3290000  done\n",
      "3300000  done\n",
      "3310000  done\n",
      "3320000  done\n",
      "3330000  done\n",
      "3340000  done\n",
      "3350000  done\n",
      "3360000  done\n",
      "3370000  done\n",
      "3380000  done\n",
      "3390000  done\n",
      "3400000  done\n",
      "3410000  done\n",
      "3420000  done\n",
      "3430000  done\n",
      "3440000  done\n",
      "3450000  done\n",
      "3460000  done\n",
      "3470000  done\n",
      "3480000  done\n",
      "3490000  done\n",
      "3500000  done\n",
      "3510000  done\n",
      "3520000  done\n",
      "3530000  done\n",
      "3540000  done\n",
      "3550000  done\n",
      "3560000  done\n",
      "3570000  done\n",
      "3580000  done\n",
      "3590000  done\n",
      "3600000  done\n",
      "3610000  done\n",
      "3620000  done\n",
      "3630000  done\n",
      "3640000  done\n",
      "3650000  done\n",
      "3660000  done\n",
      "3670000  done\n",
      "3680000  done\n",
      "3690000  done\n",
      "3700000  done\n",
      "3710000  done\n",
      "3720000  done\n",
      "3730000  done\n",
      "3740000  done\n",
      "3750000  done\n",
      "3760000  done\n",
      "3770000  done\n",
      "3780000  done\n",
      "3790000  done\n",
      "3800000  done\n",
      "3810000  done\n",
      "3820000  done\n",
      "3830000  done\n",
      "3840000  done\n",
      "3850000  done\n",
      "3860000  done\n",
      "3870000  done\n",
      "3880000  done\n",
      "3890000  done\n",
      "3900000  done\n",
      "3910000  done\n",
      "3920000  done\n",
      "3930000  done\n",
      "3940000  done\n",
      "3950000  done\n",
      "3960000  done\n",
      "3970000  done\n",
      "3980000  done\n",
      "3990000  done\n",
      "4000000  done\n",
      "4010000  done\n",
      "4020000  done\n",
      "4030000  done\n",
      "4040000  done\n",
      "4050000  done\n",
      "4060000  done\n",
      "4070000  done\n",
      "4080000  done\n",
      "4090000  done\n",
      "4100000  done\n",
      "4110000  done\n",
      "4120000  done\n",
      "4130000  done\n",
      "4140000  done\n",
      "4150000  done\n",
      "4160000  done\n",
      "4170000  done\n",
      "4180000  done\n",
      "4190000  done\n",
      "4200000  done\n",
      "4210000  done\n",
      "4220000  done\n",
      "4230000  done\n",
      "4240000  done\n",
      "4250000  done\n",
      "4260000  done\n",
      "4270000  done\n",
      "4280000  done\n",
      "4290000  done\n",
      "4300000  done\n",
      "4310000  done\n",
      "4320000  done\n",
      "4330000  done\n",
      "4340000  done\n",
      "4350000  done\n",
      "4360000  done\n",
      "4370000  done\n",
      "4380000  done\n",
      "4390000  done\n",
      "4400000  done\n",
      "4410000  done\n",
      "4420000  done\n",
      "4430000  done\n",
      "4440000  done\n",
      "4450000  done\n",
      "4460000  done\n",
      "4470000  done\n",
      "4480000  done\n",
      "4490000  done\n",
      "4500000  done\n",
      "4510000  done\n",
      "4520000  done\n",
      "4530000  done\n",
      "4540000  done\n",
      "4550000  done\n",
      "4560000  done\n",
      "4570000  done\n",
      "4580000  done\n",
      "4590000  done\n",
      "4600000  done\n",
      "4610000  done\n",
      "4620000  done\n",
      "4630000  done\n",
      "4640000  done\n",
      "4650000  done\n",
      "4660000  done\n",
      "4670000  done\n",
      "4680000  done\n",
      "4690000  done\n",
      "4700000  done\n",
      "4710000  done\n",
      "4720000  done\n",
      "4730000  done\n",
      "4740000  done\n",
      "4750000  done\n",
      "4760000  done\n",
      "4770000  done\n",
      "4780000  done\n",
      "4790000  done\n",
      "4800000  done\n",
      "4810000  done\n",
      "4820000  done\n",
      "4830000  done\n",
      "4840000  done\n",
      "4850000  done\n",
      "4860000  done\n",
      "4870000  done\n",
      "4880000  done\n",
      "4890000  done\n",
      "4900000  done\n",
      "4910000  done\n",
      "4920000  done\n",
      "4930000  done\n",
      "4940000  done\n",
      "4950000  done\n",
      "4960000  done\n",
      "4970000  done\n",
      "4980000  done\n",
      "4990000  done\n",
      "5000000  done\n",
      "5010000  done\n",
      "5020000  done\n",
      "5030000  done\n",
      "5040000  done\n",
      "5050000  done\n",
      "5060000  done\n",
      "5070000  done\n",
      "5080000  done\n",
      "5090000  done\n",
      "5100000  done\n",
      "5110000  done\n",
      "5120000  done\n",
      "5130000  done\n",
      "5140000  done\n",
      "5150000  done\n",
      "5160000  done\n",
      "5170000  done\n",
      "5180000  done\n",
      "5190000  done\n",
      "5200000  done\n",
      "5210000  done\n",
      "5220000  done\n",
      "5230000  done\n",
      "5240000  done\n",
      "5250000  done\n",
      "5260000  done\n",
      "5270000  done\n",
      "5280000  done\n",
      "5290000  done\n",
      "5300000  done\n",
      "5310000  done\n",
      "5320000  done\n",
      "5330000  done\n",
      "5340000  done\n",
      "5350000  done\n",
      "5360000  done\n",
      "5370000  done\n",
      "5380000  done\n",
      "5390000  done\n",
      "5400000  done\n",
      "5410000  done\n",
      "5420000  done\n",
      "5430000  done\n",
      "5440000  done\n",
      "5450000  done\n",
      "5460000  done\n",
      "5470000  done\n",
      "5480000  done\n",
      "5490000  done\n",
      "5500000  done\n",
      "5510000  done\n",
      "5520000  done\n",
      "5530000  done\n",
      "5540000  done\n",
      "5550000  done\n",
      "5560000  done\n",
      "5570000  done\n",
      "5580000  done\n",
      "5590000  done\n",
      "5600000  done\n",
      "5610000  done\n",
      "5620000  done\n",
      "5630000  done\n",
      "5640000  done\n",
      "5650000  done\n",
      "5660000  done\n",
      "5670000  done\n",
      "5680000  done\n",
      "5690000  done\n",
      "5700000  done\n",
      "5710000  done\n",
      "5720000  done\n",
      "5730000  done\n",
      "5740000  done\n",
      "5750000  done\n",
      "5760000  done\n",
      "5770000  done\n",
      "5780000  done\n",
      "5790000  done\n",
      "5800000  done\n",
      "5810000  done\n",
      "5820000  done\n",
      "5830000  done\n",
      "5840000  done\n",
      "5850000  done\n",
      "5860000  done\n",
      "5870000  done\n",
      "5880000  done\n",
      "5890000  done\n",
      "5900000  done\n",
      "5910000  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5920000  done\n",
      "5930000  done\n",
      "5940000  done\n",
      "5950000  done\n",
      "5960000  done\n",
      "5970000  done\n",
      "5980000  done\n",
      "5990000  done\n",
      "6000000  done\n",
      "6010000  done\n",
      "6020000  done\n",
      "6030000  done\n",
      "6040000  done\n",
      "6050000  done\n",
      "6060000  done\n",
      "6070000  done\n",
      "6080000  done\n",
      "6090000  done\n",
      "6100000  done\n",
      "6110000  done\n",
      "6120000  done\n",
      "6130000  done\n",
      "6140000  done\n",
      "6150000  done\n",
      "6160000  done\n",
      "6170000  done\n",
      "6180000  done\n",
      "6190000  done\n",
      "6200000  done\n",
      "6210000  done\n",
      "6220000  done\n",
      "6230000  done\n",
      "6240000  done\n",
      "6250000  done\n",
      "6260000  done\n",
      "6270000  done\n",
      "6280000  done\n",
      "6290000  done\n",
      "6300000  done\n",
      "6310000  done\n",
      "6320000  done\n",
      "6330000  done\n",
      "6340000  done\n",
      "6350000  done\n",
      "6360000  done\n",
      "6370000  done\n",
      "6380000  done\n",
      "6390000  done\n",
      "6400000  done\n",
      "6410000  done\n",
      "6420000  done\n",
      "6430000  done\n",
      "6440000  done\n",
      "6450000  done\n",
      "6460000  done\n",
      "6470000  done\n",
      "6480000  done\n",
      "6490000  done\n",
      "6500000  done\n",
      "6510000  done\n",
      "6520000  done\n",
      "6530000  done\n",
      "6540000  done\n",
      "6550000  done\n",
      "6560000  done\n",
      "6570000  done\n",
      "6580000  done\n",
      "6590000  done\n",
      "6600000  done\n",
      "6610000  done\n",
      "6620000  done\n",
      "6630000  done\n",
      "6640000  done\n",
      "6650000  done\n",
      "6660000  done\n",
      "6670000  done\n",
      "6680000  done\n",
      "6690000  done\n",
      "6700000  done\n",
      "6710000  done\n",
      "6720000  done\n",
      "6730000  done\n",
      "6740000  done\n",
      "6750000  done\n",
      "6760000  done\n",
      "6770000  done\n",
      "6780000  done\n",
      "6790000  done\n",
      "6800000  done\n",
      "6810000  done\n",
      "6820000  done\n",
      "6830000  done\n",
      "6840000  done\n",
      "6850000  done\n",
      "6860000  done\n",
      "6870000  done\n",
      "6880000  done\n",
      "6890000  done\n",
      "6900000  done\n",
      "6910000  done\n",
      "6920000  done\n",
      "6930000  done\n",
      "6940000  done\n",
      "6950000  done\n",
      "6960000  done\n",
      "6970000  done\n",
      "6980000  done\n",
      "6990000  done\n",
      "7000000  done\n",
      "7010000  done\n",
      "7020000  done\n",
      "7030000  done\n",
      "7040000  done\n",
      "7050000  done\n",
      "7060000  done\n",
      "7070000  done\n",
      "7080000  done\n",
      "7090000  done\n",
      "7100000  done\n",
      "7110000  done\n",
      "7120000  done\n",
      "7130000  done\n",
      "7140000  done\n",
      "7150000  done\n",
      "7160000  done\n",
      "7170000  done\n",
      "7180000  done\n",
      "7190000  done\n",
      "7200000  done\n",
      "7210000  done\n",
      "7220000  done\n",
      "7230000  done\n",
      "7240000  done\n",
      "7250000  done\n",
      "7260000  done\n",
      "7270000  done\n",
      "7280000  done\n",
      "7290000  done\n",
      "7300000  done\n",
      "7310000  done\n",
      "7320000  done\n",
      "7330000  done\n",
      "7340000  done\n",
      "7350000  done\n",
      "7360000  done\n",
      "7370000  done\n",
      "7380000  done\n",
      "7390000  done\n",
      "7400000  done\n",
      "7410000  done\n",
      "7420000  done\n",
      "7430000  done\n",
      "7440000  done\n",
      "7450000  done\n",
      "7460000  done\n",
      "7470000  done\n",
      "7480000  done\n",
      "7490000  done\n",
      "7500000  done\n",
      "7510000  done\n",
      "7520000  done\n",
      "7530000  done\n",
      "7540000  done\n",
      "7550000  done\n",
      "7560000  done\n",
      "7570000  done\n",
      "7580000  done\n",
      "7590000  done\n",
      "7600000  done\n",
      "7610000  done\n",
      "7620000  done\n",
      "7630000  done\n",
      "7640000  done\n",
      "7650000  done\n",
      "7660000  done\n",
      "7670000  done\n",
      "7680000  done\n",
      "7690000  done\n",
      "7700000  done\n",
      "7710000  done\n",
      "7720000  done\n",
      "7730000  done\n",
      "7740000  done\n",
      "7750000  done\n",
      "7760000  done\n",
      "7770000  done\n",
      "7780000  done\n",
      "7790000  done\n",
      "7800000  done\n",
      "7810000  done\n",
      "7820000  done\n",
      "7830000  done\n",
      "7840000  done\n",
      "7850000  done\n",
      "7860000  done\n",
      "7870000  done\n",
      "7880000  done\n",
      "7890000  done\n",
      "7900000  done\n",
      "7910000  done\n",
      "7920000  done\n",
      "7930000  done\n",
      "7940000  done\n",
      "7950000  done\n",
      "7960000  done\n",
      "7970000  done\n",
      "7980000  done\n",
      "7990000  done\n",
      "8000000  done\n",
      "8010000  done\n",
      "8020000  done\n",
      "8030000  done\n",
      "8040000  done\n",
      "8050000  done\n",
      "8060000  done\n",
      "8070000  done\n",
      "8080000  done\n",
      "8090000  done\n",
      "8100000  done\n",
      "8110000  done\n",
      "8120000  done\n",
      "8130000  done\n",
      "8140000  done\n",
      "8150000  done\n",
      "8160000  done\n",
      "8170000  done\n",
      "8180000  done\n",
      "8190000  done\n",
      "8200000  done\n",
      "8210000  done\n",
      "8220000  done\n",
      "8230000  done\n",
      "8240000  done\n",
      "8250000  done\n",
      "8260000  done\n",
      "8270000  done\n",
      "8280000  done\n",
      "8290000  done\n",
      "8300000  done\n",
      "8310000  done\n",
      "8320000  done\n",
      "8330000  done\n",
      "8340000  done\n",
      "8350000  done\n",
      "8360000  done\n",
      "8370000  done\n",
      "8380000  done\n",
      "8390000  done\n",
      "8400000  done\n",
      "8410000  done\n",
      "8420000  done\n",
      "8430000  done\n",
      "8440000  done\n",
      "8450000  done\n",
      "8460000  done\n",
      "8470000  done\n",
      "8480000  done\n",
      "8490000  done\n",
      "8500000  done\n",
      "8510000  done\n",
      "8520000  done\n",
      "8530000  done\n",
      "8540000  done\n",
      "8550000  done\n",
      "8560000  done\n",
      "8570000  done\n",
      "8580000  done\n",
      "8590000  done\n",
      "8600000  done\n",
      "8610000  done\n",
      "8620000  done\n",
      "8630000  done\n",
      "8640000  done\n",
      "8650000  done\n",
      "8660000  done\n",
      "8670000  done\n",
      "8680000  done\n",
      "8690000  done\n",
      "8700000  done\n",
      "8710000  done\n",
      "8720000  done\n",
      "8730000  done\n",
      "8740000  done\n",
      "8750000  done\n",
      "8760000  done\n",
      "8770000  done\n",
      "8780000  done\n",
      "8790000  done\n",
      "8800000  done\n",
      "8810000  done\n",
      "8820000  done\n",
      "8830000  done\n",
      "8840000  done\n",
      "8850000  done\n",
      "8860000  done\n",
      "8870000  done\n",
      "8880000  done\n",
      "8890000  done\n",
      "8900000  done\n",
      "8910000  done\n",
      "8920000  done\n",
      "8930000  done\n",
      "8940000  done\n",
      "8950000  done\n",
      "8960000  done\n",
      "8970000  done\n",
      "8980000  done\n",
      "8990000  done\n",
      "9000000  done\n",
      "9010000  done\n",
      "9020000  done\n",
      "9030000  done\n",
      "9040000  done\n",
      "9050000  done\n",
      "9060000  done\n",
      "9070000  done\n",
      "9080000  done\n",
      "9090000  done\n",
      "9100000  done\n",
      "9110000  done\n",
      "9120000  done\n",
      "9130000  done\n",
      "9140000  done\n",
      "9150000  done\n",
      "9160000  done\n",
      "9170000  done\n",
      "9180000  done\n",
      "9190000  done\n",
      "9200000  done\n",
      "9210000  done\n",
      "9220000  done\n",
      "9230000  done\n",
      "9240000  done\n",
      "9250000  done\n",
      "9260000  done\n",
      "9270000  done\n",
      "9280000  done\n",
      "9290000  done\n",
      "9300000  done\n",
      "9310000  done\n",
      "9320000  done\n",
      "9330000  done\n",
      "9340000  done\n",
      "9350000  done\n",
      "9360000  done\n",
      "9370000  done\n",
      "9380000  done\n",
      "9390000  done\n",
      "9400000  done\n",
      "9410000  done\n",
      "9420000  done\n",
      "9430000  done\n",
      "9440000  done\n",
      "9450000  done\n",
      "9460000  done\n",
      "9470000  done\n",
      "9480000  done\n",
      "9490000  done\n",
      "9500000  done\n",
      "9510000  done\n",
      "9520000  done\n",
      "9530000  done\n",
      "9540000  done\n",
      "9550000  done\n",
      "9560000  done\n",
      "9570000  done\n",
      "9580000  done\n",
      "9590000  done\n",
      "9600000  done\n",
      "9610000  done\n",
      "9620000  done\n",
      "9630000  done\n",
      "9640000  done\n",
      "9650000  done\n",
      "9660000  done\n",
      "9670000  done\n",
      "9680000  done\n",
      "9690000  done\n",
      "9700000  done\n",
      "9710000  done\n",
      "9720000  done\n",
      "9730000  done\n",
      "9740000  done\n",
      "9750000  done\n",
      "9760000  done\n",
      "9770000  done\n",
      "9780000  done\n",
      "9790000  done\n",
      "9800000  done\n",
      "9810000  done\n",
      "9820000  done\n",
      "9830000  done\n",
      "9840000  done\n",
      "9850000  done\n",
      "9860000  done\n",
      "9870000  done\n",
      "9880000  done\n",
      "9890000  done\n",
      "9900000  done\n",
      "9910000  done\n",
      "9920000  done\n",
      "9930000  done\n",
      "9940000  done\n",
      "9950000  done\n",
      "9960000  done\n",
      "9970000  done\n",
      "9980000  done\n",
      "9990000  done\n",
      "10000000  done\n",
      "10010000  done\n",
      "10020000  done\n",
      "10030000  done\n",
      "10040000  done\n",
      "10050000  done\n",
      "10060000  done\n",
      "10070000  done\n",
      "10080000  done\n",
      "10090000  done\n",
      "10100000  done\n",
      "10110000  done\n",
      "10120000  done\n",
      "10130000  done\n",
      "10140000  done\n",
      "10150000  done\n",
      "10160000  done\n",
      "10170000  done\n",
      "10180000  done\n",
      "10190000  done\n",
      "10200000  done\n",
      "10210000  done\n",
      "10220000  done\n",
      "10230000  done\n",
      "10240000  done\n",
      "10250000  done\n",
      "10260000  done\n",
      "10270000  done\n",
      "10280000  done\n",
      "10290000  done\n",
      "10300000  done\n",
      "10310000  done\n",
      "10320000  done\n",
      "10330000  done\n",
      "10340000  done\n",
      "10350000  done\n",
      "10360000  done\n",
      "10370000  done\n",
      "10380000  done\n",
      "10390000  done\n",
      "10400000  done\n",
      "10410000  done\n",
      "10420000  done\n",
      "10430000  done\n",
      "10440000  done\n",
      "10450000  done\n",
      "10460000  done\n",
      "10470000  done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#[\"Cow\",\"Cat\",\"Mouse\",\"Pig\",\"Rhesus\",\"Microbat\",\"Elephant\",\"Rabbit\",\"Shrew\"]\n",
    "print \"total length:\",len(trainingIndices)\n",
    "#trainingNucleotide=[]\n",
    "for enum,i in enumerate(trainingIndices):\n",
    "    if enum%10000==0:\n",
    "        print enum,\" done\"\n",
    "    temp=i[0:10]\n",
    "    for index,k in enumerate(temp):\n",
    "        #print k\n",
    "        if index%10==0:\n",
    "            if k==-19 or k==\"-19\":\n",
    "                humanNucleotide='-'\n",
    "            else:\n",
    "                indexStr = str(k)\n",
    "                firstUnderscore= indexStr.index('_')\n",
    "                secondUnderscore= indexStr.rfind('_')\n",
    "                seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                intIndex=int(indexStr[0:firstUnderscore])\n",
    "                strand=int(indexStr[secondUnderscore+1:])\n",
    "                if strand==1:\n",
    "                    humanNucleotide=humanSeqFasta[humanSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                else:\n",
    "                    posNucleotide=humanSeqFasta[humanSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    humanNucleotide=getComplementBasePair(posNucleotide)\n",
    "            trainingIndices[enum][index]=humanNucleotide\n",
    "            #temp.append(humanNucleotide)\n",
    "        if index%10==1:\n",
    "            try:\n",
    "                if k==-19 or k==\"-19\":\n",
    "                    cowNucleotide='-'\n",
    "                else:\n",
    "                    indexStr = str(k)\n",
    "                    firstUnderscore= indexStr.index('_')\n",
    "                    secondUnderscore= indexStr.rfind('_')\n",
    "                    seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                    intIndex=int(indexStr[0:firstUnderscore])\n",
    "                    strand=int(indexStr[secondUnderscore+1:])\n",
    "                    if strand==1:\n",
    "                        cowNucleotide=cowSeqFasta[cowSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    else:\n",
    "                        posNucleotide=cowSeqFasta[cowSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                        cowNucleotide=getComplementBasePair(posNucleotide)\n",
    "                trainingIndices[enum][index]=cowNucleotide\n",
    "            except:\n",
    "                continue\n",
    "            #temp.append(cowNucleotide)\n",
    "        if index%10==2:\n",
    "            if k==-19 or k==\"-19\":\n",
    "                catNucleotide='-'\n",
    "            else:\n",
    "                indexStr = str(k)\n",
    "                firstUnderscore= indexStr.index('_')\n",
    "                secondUnderscore= indexStr.rfind('_')\n",
    "                seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                intIndex=int(indexStr[0:firstUnderscore])\n",
    "                strand=int(indexStr[secondUnderscore+1:])\n",
    "                if strand==1:\n",
    "                    catNucleotide=catSeqFasta[catSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                else:\n",
    "                    posNucleotide=catSeqFasta[catSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    catNucleotide=getComplementBasePair(posNucleotide)\n",
    "            trainingIndices[enum][index]=catNucleotide\n",
    "            #temp.append(catNucleotide)\n",
    "        if index%10==3:\n",
    "            if k==-19 or k==\"-19\":\n",
    "                mouseNucleotide='-'\n",
    "            else:\n",
    "                indexStr = str(k)\n",
    "                firstUnderscore= indexStr.index('_')\n",
    "                secondUnderscore= indexStr.rfind('_')\n",
    "                seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                intIndex=int(indexStr[0:firstUnderscore])\n",
    "                strand=int(indexStr[secondUnderscore+1:])\n",
    "                if strand==1:\n",
    "                    mouseNucleotide=mouseSeqFasta[mouseSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                else:\n",
    "                    posNucleotide=mouseSeqFasta[mouseSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    mouseNucleotide=getComplementBasePair(posNucleotide)\n",
    "            trainingIndices[enum][index]=mouseNucleotide\n",
    "            #temp.append(mouseNucleotide)\n",
    "        if index%10==4:\n",
    "            if k==-19 or k==\"-19\":\n",
    "                pigNucleotide='-'\n",
    "            else:\n",
    "                indexStr = str(k)\n",
    "                firstUnderscore= indexStr.index('_')\n",
    "                secondUnderscore= indexStr.rfind('_')\n",
    "                seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                intIndex=int(indexStr[0:firstUnderscore])\n",
    "                strand=int(indexStr[secondUnderscore+1:])\n",
    "                if strand==1:\n",
    "                    pigNucleotide=pigSeqFasta[pigSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                else:\n",
    "                    posNucleotide=pigSeqFasta[pigSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    pigNucleotide=getComplementBasePair(posNucleotide)\n",
    "            trainingIndices[enum][index]=pigNucleotide\n",
    "            #temp.append(pigNucleotide)\n",
    "        if index%10==5:\n",
    "            if k==-19 or k==\"-19\":\n",
    "                rhesusNucleotide='-'\n",
    "            else:\n",
    "                indexStr = str(k)\n",
    "                firstUnderscore= indexStr.index('_')\n",
    "                secondUnderscore= indexStr.rfind('_')\n",
    "                seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                intIndex=int(indexStr[0:firstUnderscore])\n",
    "                strand=int(indexStr[secondUnderscore+1:])\n",
    "                if strand==1:\n",
    "                    rhesusNucleotide=rhesusSeqFasta[rhesusSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                else:\n",
    "                    posNucleotide=rhesusSeqFasta[rhesusSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    rhesusNucleotide=getComplementBasePair(posNucleotide)\n",
    "            trainingIndices[enum][index]=rhesusNucleotide\n",
    "        if index%10==6:\n",
    "            if k==-19 or k==\"-19\":\n",
    "                microbatNucleotide='-'\n",
    "            else:\n",
    "                indexStr = str(k)\n",
    "                firstUnderscore= indexStr.index('_')\n",
    "                secondUnderscore= indexStr.rfind('_')\n",
    "                seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                intIndex=int(indexStr[0:firstUnderscore])\n",
    "                strand=int(indexStr[secondUnderscore+1:])\n",
    "                if strand==1:\n",
    "                    microbatNucleotide=microbatSeqFasta[microbatSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                else:\n",
    "                    posNucleotide=microbatSeqFasta[microbatSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    microbatNucleotide=getComplementBasePair(posNucleotide)\n",
    "            trainingIndices[enum][index]=microbatNucleotide\n",
    "        if index%10==7:\n",
    "            if k==-19 or k==\"-19\":\n",
    "                elephantNucleotide='-'\n",
    "            else:\n",
    "                indexStr = str(k)\n",
    "                firstUnderscore= indexStr.index('_')\n",
    "                secondUnderscore= indexStr.rfind('_')\n",
    "                seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                intIndex=int(indexStr[0:firstUnderscore])\n",
    "                strand=int(indexStr[secondUnderscore+1:])\n",
    "                if strand==1:\n",
    "                    elephantNucleotide=elephantSeqFasta[elephantSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                else:\n",
    "                    posNucleotide=elephantSeqFasta[elephantSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    elephantNucleotide=getComplementBasePair(posNucleotide)\n",
    "            trainingIndices[enum][index]=elephantNucleotide\n",
    "        if index%10==8:\n",
    "            if k==-19 or k==\"-19\":\n",
    "                rabbitNucleotide='-'\n",
    "            else:\n",
    "                indexStr = str(k)\n",
    "                firstUnderscore= indexStr.index('_')\n",
    "                secondUnderscore= indexStr.rfind('_')\n",
    "                seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                intIndex=int(indexStr[0:firstUnderscore])\n",
    "                strand=int(indexStr[secondUnderscore+1:])\n",
    "                if strand==1:\n",
    "                    rabbitNucleotide=rabbitSeqFasta[rabbitSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                else:\n",
    "                    posNucleotide=rabbitSeqFasta[rabbitSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    rabbitNucleotide=getComplementBasePair(posNucleotide)\n",
    "            trainingIndices[enum][index]=rabbitNucleotide\n",
    "        if index%10==9:\n",
    "            if k==-19 or k==\"-19\":\n",
    "                shrewNucleotide='-'\n",
    "            else:\n",
    "                indexStr = str(k)\n",
    "                firstUnderscore= indexStr.index('_')\n",
    "                secondUnderscore= indexStr.rfind('_')\n",
    "                seqNameInIndex=indexStr[firstUnderscore+1:secondUnderscore]\n",
    "                intIndex=int(indexStr[0:firstUnderscore])\n",
    "                strand=int(indexStr[secondUnderscore+1:])\n",
    "                if strand==1:\n",
    "                    shrewNucleotide=shrewSeqFasta[shrewSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                else:\n",
    "                    posNucleotide=shrewSeqFasta[shrewSeqName.index(seqNameInIndex)][intIndex].upper()\n",
    "                    shrewNucleotide=getComplementBasePair(posNucleotide)\n",
    "            trainingIndices[enum][index]=shrewNucleotide\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10471799\n",
      "['C', 'C', 'A', '-', '-', 'A', 'C', 'A', 'C', '-', 3634915]\n",
      "['C', u'2652956_Cow.chr22.2.1.1.1_-1', 'C', '-', 'T', 'C', 'A', '-', 'C', '-', 4276219]\n"
     ]
    }
   ],
   "source": [
    "print len(trainingIndices)\n",
    "print trainingIndices[0]\n",
    "for i in trainingIndices:\n",
    "    for j in i[0:10]:\n",
    "        if('_' in j):\n",
    "            print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "10471798\n"
     ]
    }
   ],
   "source": [
    "for indx,i in enumerate(trainingIndices):\n",
    "    if(i[1] =='2652956_Cow.chr22.2.1.1.1_-1'):\n",
    "        print 'hello'\n",
    "        trainingIndices.remove(i)\n",
    "print len(trainingIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'A', '-', '-', 'A', 'C', 'A', 'C', '-', 3634915]\n",
      "['G', 'G', 'G', 'G', 'G', 'G', 'A', 'G', 'G', '-', 7224579]\n"
     ]
    }
   ],
   "source": [
    "print trainingIndices[0]#checking the data format. the 10th index is human index\n",
    "print trainingIndices[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary with the human index as key and everything else as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingIndicesDic={}\n",
    "for i in trainingIndices:\n",
    "    trainingIndicesDic[i[10]]=i[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(len(trainingIndicesDic)==len(trainingIndices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Mismatch bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another list of features that are for match mismatch bits w.r.t. to humans. if the nucleotide matches with respect to human then set match bit to 1. else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matchMismatchBit = []\n",
    "for i in trainingIndices:\n",
    "    temp = []\n",
    "    humanNucleotide = i[0]\n",
    "    for j in i[1:10]:\n",
    "        if j==humanNucleotide:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    temp.append(i[10])\n",
    "    matchMismatchBit.append(temp)\n",
    "assert(len(matchMismatchBit)==len(trainingIndices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10471798\n",
      "10471798\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 3634915]\n",
      "['C', 'C', 'A', '-', '-', 'A', 'C', 'A', 'C', '-', 3634915]\n"
     ]
    }
   ],
   "source": [
    "print len(matchMismatchBit)\n",
    "print len(trainingIndices)\n",
    "print matchMismatchBit[0]\n",
    "print trainingIndices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matchMismatchDic={}\n",
    "for i in matchMismatchBit:\n",
    "    matchMismatchDic[i[9]]=i[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del trainingIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del matchMismatchBit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancestor Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "AncestorNucleotide=[]\n",
    "with open(\"TrainingIndices10Species_onlyHumanwithAncestors.json\", 'r') as f:\n",
    "    AncestorNucleotide = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'0_Human.chr22_1', [u'G', u'G', u'-', u'G', u'G', u'G', u'G', u'G', u'G']]\n"
     ]
    }
   ],
   "source": [
    "print AncestorNucleotide[0]#checking the data format loaded list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AncestorSequenceList=[]\n",
    "for i in AncestorNucleotide:\n",
    "    indexStr=str(i[0])\n",
    "    firstUnderscore=indexStr.index('_')\n",
    "    intIndex=int(indexStr[0:firstUnderscore])\n",
    "    i[1].append(intIndex)\n",
    "    AncestorSequenceList.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'G', u'G', u'-', u'G', u'G', u'G', u'G', u'G', u'G', 0], [u'A', u'A', u'A', u'A', u'A', u'A', u'A', u'A', u'A', 10000000], [u'A', u'A', u'A', u'A', u'A', u'A', u'A', u'A', u'A', 10000001], [u'A', u'A', u'A', u'A', u'A', u'A', u'A', u'A', u'A', 10000002], [u'A', u'A', u'A', u'A', u'A', u'A', u'A', u'A', u'A', 10000003]]\n"
     ]
    }
   ],
   "source": [
    "assert(len(AncestorSequenceList)==len(AncestorNucleotide))\n",
    "print AncestorSequenceList[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del AncestorNucleotide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancestor Transition Transversion bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each branch in the phylogeny tree, we would see if their is a transition/transversion/indel along that branch going from the ancestor of that branch to the descendant. We have used 6 bits for encoding, 1 bit for indicating transition, 1 bit for indicating transversion,1 bit for indicating insertion, 1 bit for indicating deletion, 1 bit if no change in the branch, 1 bit if both the nucleotides are gaps. The encoding scheme is defined below\n",
    "#transitionbit,transversionbit,Match,Insertion,Deletion,Gap\n",
    "\n",
    "<table>\n",
    "<tr><td>Transition Bit</td><td>Transversion Bit</td><td>Same Nucleotide Bit</td><td>Insertion Bit</td><td>Deletion Bit</td><td>Gap Bit</td><td>Description</td></tr>\n",
    "<tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>No change in that branch</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>Transition along that branch</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>Transversion along that branch</td></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>Insertion along that branch</td></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>Deletion along that branch</td></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>Both gaps along that branch</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transition_transversion_features(humanNucleotide,otherSpeciesNucleotide):\n",
    "    #return format transitionbit,transversionbit,Match,Insertion,Deletion,Gap\n",
    "    if (humanNucleotide==otherSpeciesNucleotide):\n",
    "        return 0,0,1,0,0,0\n",
    "    elif (otherSpeciesNucleotide=='-' and humanNucleotide=='-'):\n",
    "        return 0,0,0,0,0,1\n",
    "    elif (otherSpeciesNucleotide=='-' and humanNucleotide!='-'):\n",
    "        return 0,0,0,1,0,0\n",
    "    elif (otherSpeciesNucleotide!='-' and humanNucleotide=='-'):\n",
    "        return 0,0,0,0,1,0\n",
    "    elif ((humanNucleotide=='A' and otherSpeciesNucleotide=='G') or (humanNucleotide=='G' and otherSpeciesNucleotide=='A')):\n",
    "        return 1,0,0,0,0,0\n",
    "    elif ((humanNucleotide=='C' and otherSpeciesNucleotide=='T') or (humanNucleotide=='T' and otherSpeciesNucleotide=='C')):\n",
    "        return 1,0,0,0,0,0\n",
    "    else:\n",
    "        return 0,1,0,0,0,0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ancestor data; we need to match each ancestor data with the corresponding human index and make a list for this feature in the same order in which the data is present in combined list. The combined list contains the human index in the 10th position. we will use that to make a connection between these 2 lists. Firstly we see that the human index in ancestor nucleotide contains the species and the chromosome name. Since we know that for human there is only 1 unique sequence name we will remove everything and keep only the index intact in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10471798\n",
      "10542728\n",
      "[u'G', u'G', u'-', u'G', u'G', u'G', u'G', u'G', u'G', 0]\n",
      "0\n",
      "G\n",
      "G\n",
      "G\n",
      "-\n",
      "G\n",
      "G\n",
      "-\n",
      "-\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "print len(AncestorSequenceList[0])\n",
    "print len(trainingIndicesDic)\n",
    "print len(AncestorSequenceList)\n",
    "print AncestorSequenceList[0]\n",
    "print AncestorSequenceList[0][-1]\n",
    "for i in range(9):\n",
    "    print trainingIndicesDic[0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[\"Human\",\"Cow\",\"Cat\",\"Mouse\",\"Pig\",\"Rhesus\",\"Microbat\",\"Elephant\",\"Rabbit\",\"Shrew\"]\n",
    "#[LC_P[i],LC_M_C_P[i],LM_R[i],LC_M[i],LC_M_C_P_S[i],LH_R_M_R[i],LH_R[i],LC_M_C_P_S_H_R_M_R[i],Lhg18[i]]\n",
    "#format transitionbit,transversionbit,Match,Insertion,Deletion,Gap\n",
    "AncestorTransitionTransversionBits = []\n",
    "AncestorTransitionTransversionSum=[]\n",
    "for i in AncestorSequenceList:\n",
    "    if i[9] not in trainingIndicesDic:\n",
    "        continue\n",
    "    transitionSum,transversionSum,matchSum,insertionSum,delSum,gapSum=0,0,0,0,0,0\n",
    "    temp = []\n",
    "    #temp = i[0:20]\n",
    "    #H - HR\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][0],i[-4])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #R - HR\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][5],i[-4])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #Cow - CowPig\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][1],i[-10])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #Pig - CowPig\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][4],i[-10])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #Cat - CatMicrobat\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][2],i[-7])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #Microbat - CatMicrobat\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][6],i[-7])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #Mouse - MouseRabbit\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][3],i[-8])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #Rabbit - MouseRabbit\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][8],i[-9])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #CM - CMCP\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(i[-7],i[-9])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #CP - CMCP\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(i[-10],i[-9])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #CMCP - CMCPS\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(i[-9],i[-6])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #Shrew - CMCPS\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][9],i[-6])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #HR - HRMR\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(i[-4],i[-5])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #MR - HRMR\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(i[-8],i[-5])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #CMCPS - CMCPSHRMR\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(i[-6],i[-3])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #HRMR - CMCPSHRMR\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(i[-5],i[-3])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #CMCPSHRMR - Hg18\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(i[-3],i[-2])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    #Elephant - Hg18\n",
    "    x,y,z,a,b,c = get_transition_transversion_features(trainingIndicesDic[i[9]][7],i[-2])\n",
    "    temp.append(x)\n",
    "    temp.append(y)\n",
    "    temp.append(z)\n",
    "    temp.append(a)\n",
    "    temp.append(b)\n",
    "    temp.append(c)\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==0 and c==1:\n",
    "        gapSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==0 and b==1 and c==0:\n",
    "        delSum+=1\n",
    "    if x==0 and y==0 and z==0 and a==1 and b==0 and c==0:\n",
    "        insertionSum+=1\n",
    "    if x==0 and y==0 and z==1 and a==0 and b==0 and c==0:\n",
    "        matchSum+=1\n",
    "    if x==1 and y==0 and z==0 and a==0 and b==0 and c==0:\n",
    "        transitionSum+=1\n",
    "    if x==0 and y==1 and z==0 and a==0 and b==0 and c==0:\n",
    "        transversionSum+=1\n",
    "    temp.append(i[9])\n",
    "    AncestorTransitionTransversionBits.append(temp)    \n",
    "    AncestorTransitionTransversionSum.append([transitionSum,transversionSum,matchSum,insertionSum,delSum,gapSum,i[9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print AncestorTransitionTransversionBits[1000]\n",
    "print AncestorTransitionTransversionSum[1000]\n",
    "print len(AncestorTransitionTransversionBits[0])\n",
    "print len(AncestorTransitionTransversionSum[0])\n",
    "print len(AncestorTransitionTransversionBits[1000][0:108])\n",
    "print AncestorTransitionTransversionBits[1000][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AncestorTransitionTransversionBitsDic={}\n",
    "for i in AncestorTransitionTransversionBits:\n",
    "    AncestorTransitionTransversionBitsDic[i[-1]]=i[0:108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AncestorTransitionTransversionSumDic={}\n",
    "for i in AncestorTransitionTransversionSum:\n",
    "    AncestorTransitionTransversionSumDic[i[-1]]=i[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AncestorSequenceListDic={}\n",
    "for i in AncestorSequenceList:\n",
    "    AncestorSequenceListDic[i[-1]]=i[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DesampledLabelListDic={}\n",
    "for i in DesampledLabelList:\n",
    "    DesampledLabelListDic[i[0]]=i[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print trainingIndicesDic[0]\n",
    "#print DesampledLabelListDic[2345]\n",
    "print AncestorTransitionTransversionBitsDic[0]\n",
    "print AncestorTransitionTransversionSumDic[0]\n",
    "print matchMismatchDic[0]\n",
    "print len(trainingIndicesDic[0])\n",
    "#print len(DesampledLabelListDic[0])\n",
    "print len(AncestorTransitionTransversionBitsDic[0])\n",
    "print len(AncestorTransitionTransversionSumDic[0])\n",
    "print len(matchMismatchDic[0])\n",
    "print len(trainingIndicesDic)\n",
    "print len(DesampledLabelListDic)\n",
    "print len(AncestorTransitionTransversionBitsDic)\n",
    "print len(AncestorTransitionTransversionSumDic)\n",
    "print len(matchMismatchDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del DesampledLabelList,AncestorSequenceList,AncestorTransitionTransversionBits,AncestorTransitionTransversionSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del humanSeqFasta,humanSeqName,rabbitSeqFasta,rabbitSeqName,rhesusSeqFasta,rhesusSeqName,pigSeqFasta,pigSeqName,catSeqFasta,catSeqName,cowSeqName,cowSeqFasta,shrewSeqFasta,shrewSeqName,elephantSeqFasta,elephantSeqName,microbatSeqFasta,microbatSeqName,mouseSeqFasta,mouseSeqName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize we have the following dictionaries with the human index as key in each dictionary which containt the corresponding features.\n",
    "<table>\n",
    "<tr><th>List Name</th><th>Feature Description</th></tr>\n",
    "<tr><td>trainingIndicesDic</td><td>The columns nucleotides</td></tr>\n",
    "<tr><td>DesampledLabelListDic</td><td>The labels of the corresponding nucleotides</td></tr>\n",
    "<tr><td>AncestorTransitionTransversionBitsDic</td><td>The transition transversion bits of every edge in the phylogeny tree</td></tr>\n",
    "<tr><td>AncestorTransitionTransversionSumDic</td><td>The sum of transition transverstion bits of every edge in the tree</td></tr>\n",
    "<tr><td>matchMismatchDic</td><td>The match mismatch bits of other species with respect to humans</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set 1(Only Sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the dictionary trainingIndicesDic and ofcourse the labels DesampledLabelListDic for this feature. We create the data set which can take the parameter window size, then ensure the class balance of the label set and then use a classifier to get the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noOfNeighborsOnOneSide=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records: 4034749\n",
      "Processed  100000  records.\n",
      "Processed  200000  records.\n",
      "Processed  300000  records.\n",
      "Processed  400000  records.\n",
      "Processed  500000  records.\n",
      "Processed  600000  records.\n",
      "Processed  700000  records.\n",
      "Processed  800000  records.\n",
      "Processed  900000  records.\n",
      "Processed  1000000  records.\n",
      "Processed  1100000  records.\n",
      "Processed  1200000  records.\n",
      "Processed  1300000  records.\n",
      "Processed  1400000  records.\n",
      "Processed  1500000  records.\n",
      "Processed  1600000  records.\n",
      "Processed  1700000  records.\n",
      "Processed  1800000  records.\n",
      "Processed  1900000  records.\n",
      "Processed  2000000  records.\n",
      "Processed  2100000  records.\n",
      "Processed  2200000  records.\n",
      "Processed  2300000  records.\n",
      "Processed  2400000  records.\n",
      "Processed  2500000  records.\n",
      "Processed  2600000  records.\n",
      "Processed  2700000  records.\n",
      "Processed  2800000  records.\n",
      "Processed  2900000  records.\n",
      "Processed  3000000  records.\n",
      "Processed  3100000  records.\n",
      "Processed  3200000  records.\n",
      "Processed  3300000  records.\n",
      "Processed  3400000  records.\n",
      "Processed  3500000  records.\n",
      "Processed  3600000  records.\n",
      "Processed  3700000  records.\n",
      "Processed  3800000  records.\n",
      "Processed  3900000  records.\n",
      "Processed  4000000  records.\n"
     ]
    }
   ],
   "source": [
    "TrainingX=[]\n",
    "TrainingY=[]\n",
    "print \"Total Records:\",len(DesampledLabelListDic)\n",
    "for enum,key in enumerate(DesampledLabelListDic):\n",
    "    if enum%100000==0 and enum!=0:\n",
    "        print \"Processed \",enum,\" records.\"\n",
    "    keyDoesntExist = 0\n",
    "    tempList=[]\n",
    "    for j in reversed(range(1,(noOfNeighborsOnOneSide+1))):\n",
    "        if key-j in trainingIndicesDic:\n",
    "            tempList+=(trainingIndicesDic[key-j])\n",
    "        else:\n",
    "            keyDoesntExist = 1\n",
    "            break\n",
    "    if keyDoesntExist == 1:\n",
    "        continue\n",
    "    if key in trainingIndicesDic:\n",
    "        tempList+=(trainingIndicesDic[key])\n",
    "    else:\n",
    "        continue\n",
    "    for k in range(1,(noOfNeighborsOnOneSide+1)):\n",
    "        if key+k in trainingIndicesDic:\n",
    "            tempList+=(trainingIndicesDic[key+k])\n",
    "        else:\n",
    "            keyDoesntExist=1\n",
    "            break\n",
    "    if keyDoesntExist == 1:\n",
    "        continue\n",
    "    TrainingX.append(tempList)\n",
    "    TrainingY.append(DesampledLabelListDic[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4034748\n",
      "4034748\n"
     ]
    }
   ],
   "source": [
    "print len(TrainingX)\n",
    "print len(TrainingY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need some kind of encoding to encode the nucleotides in the TrainingX. We will use one hot encoding here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n"
     ]
    }
   ],
   "source": [
    "print TrainingX[0]#print to check the format of data once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function for one hot encoding and getting IndicesOneHotEncoded list at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_one_hot_encoding(shapelist,NucleoHash):\n",
    "    x=[]\n",
    "    for count,z in enumerate(shapelist):\n",
    "        if(count%100000==0):\n",
    "            print count\n",
    "        p = []\n",
    "        for j,k in enumerate(z):\n",
    "            if(is_number(k)):\n",
    "                p.append(k)\n",
    "            else:\n",
    "                p+=NucleoHash[k]\n",
    "        #p = [int(i) for i in p]\n",
    "        x.append(p)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    " \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n"
     ]
    }
   ],
   "source": [
    "NucleoHash={'A':[1,0,0,0,0],'C':[0,1,0,0,0],'G':[0,0,1,0,0],'T':[0,0,0,1,0],'-':[0,0,0,0,1]}\n",
    "#NucleoHash={'A':100,'C':101,'G':102,'T':103,'-':104}\n",
    "TrainingXOneHotEncoded =[]\n",
    "TrainingXOneHotEncoded = generate_one_hot_encoding(TrainingX,NucleoHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "4034748\n",
      "4034748\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#print IndicesList[0]\n",
    "print TrainingXOneHotEncoded[0]\n",
    "print len(TrainingXOneHotEncoded)\n",
    "print len(TrainingY)\n",
    "print TrainingY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del TrainingX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sckit Learn Train-Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell just splits the X_training and Y_training into 70:30 percent as training and testing data respectively. It also shuffles the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(TrainingXOneHotEncoded, TrainingY, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the random forest classifier here and we will only use 1 million records for training and 300k for testing as we feel that amount of data is sufficient to select the proper features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10,verbose=1,max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# clf = DecisionTreeClassifier(max_depth=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   49.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the classifier is fit, lets predict for 300k records in test and then predict for the training data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    8.4s finished\n"
     ]
    }
   ],
   "source": [
    "y_train_pred=clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 4 0 4 2 0 4 0 2]\n",
      "[[3], [1], [2], [2], [2], [0], [0], [2], [4], [2]]\n"
     ]
    }
   ],
   "source": [
    "print y_pred[0:10]\n",
    "print y_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function outputs the accuracy when you input two lists the predicted and the other list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(yPred,yTest):\n",
    "    if len(yPred)!=len(yTest):\n",
    "        print \"yPred and yTest have different dimensions.\"\n",
    "        return\n",
    "    overallCorrect=0\n",
    "    stringTable=\"<table><tr><th>True_Predicted</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th><th>Percentage</th></tr>\"\n",
    "    #classxy read as true class x predicted class y\n",
    "    class00,class01,class02,class03,class04=0,0,0,0,0\n",
    "    class10,class11,class12,class13,class14=0,0,0,0,0\n",
    "    class20,class21,class22,class23,class24=0,0,0,0,0\n",
    "    class30,class31,class32,class33,class34=0,0,0,0,0\n",
    "    class40,class41,class42,class43,class44=0,0,0,0,0\n",
    "    for enum1,i in enumerate(yPred):\n",
    "        for enum2,j in enumerate(i):\n",
    "            if j==yTest[enum1][enum2]:\n",
    "                overallCorrect+=1\n",
    "            if yTest[enum1][enum2]==0:\n",
    "                if j==0:\n",
    "                    class00+=1\n",
    "                if j==1:\n",
    "                    class01+=1\n",
    "                if j==2:\n",
    "                    class02+=1\n",
    "                if j==3:\n",
    "                    class03+=1\n",
    "                if j==4:\n",
    "                    class04+=1\n",
    "            if yTest[enum1][enum2]==1:\n",
    "                if j==0:\n",
    "                    class10+=1\n",
    "                if j==1:\n",
    "                    class11+=1\n",
    "                if j==2:\n",
    "                    class12+=1\n",
    "                if j==3:\n",
    "                    class13+=1\n",
    "                if j==4:\n",
    "                    class14+=1\n",
    "            if yTest[enum1][enum2]==2:\n",
    "                if j==0:\n",
    "                    class20+=1\n",
    "                if j==1:\n",
    "                    class21+=1\n",
    "                if j==2:\n",
    "                    class22+=1\n",
    "                if j==3:\n",
    "                    class23+=1\n",
    "                if j==4:\n",
    "                    class24+=1\n",
    "            if yTest[enum1][enum2]==3:\n",
    "                if j==0:\n",
    "                    class30+=1\n",
    "                if j==1:\n",
    "                    class31+=1\n",
    "                if j==2:\n",
    "                    class32+=1\n",
    "                if j==3:\n",
    "                    class33+=1\n",
    "                if j==4:\n",
    "                    class34+=1\n",
    "            if yTest[enum1][enum2]==4:\n",
    "                if j==0:\n",
    "                    class40+=1\n",
    "                if j==1:\n",
    "                    class41+=1\n",
    "                if j==2:\n",
    "                    class42+=1\n",
    "                if j==3:\n",
    "                    class43+=1\n",
    "                if j==4:\n",
    "                    class44+=1\n",
    "    accuracyPercentage=overallCorrect*1.0/(len(yPred[0])*len(yPred))\n",
    "    print \"Overall Accuracy is\",accuracyPercentage\n",
    "    print \"Class 0:\",class00,class01,class02,class03,class04,class00+class01+class02+class03+class04,class00*1.0/(class00+class01+class02+class03+class04)\n",
    "    print \"Class 1:\",class10,class11,class12,class13,class14,class10+class11+class12+class13+class14,class11*1.0/(class10+class11+class12+class13+class14)\n",
    "    print \"Class 2:\",class20,class21,class22,class23,class24,class20+class21+class22+class23+class24,class22*1.0/(class20+class21+class22+class23+class24)\n",
    "    print \"Class 3:\",class30,class31,class32,class33,class34,class30+class31+class32+class33+class34,class33*1.0/(class30+class31+class32+class33+class34)\n",
    "    print \"Class 4:\",class40,class41,class42,class43,class44,class40+class41+class42+class43+class44,class44*1.0/(class40+class41+class42+class43+class44)\n",
    "    stringTable+=\"<tr><td>\"+\"Class 0\"+\"</td><td>\"+str(class00)+\"</td><td>\"+str(class01)+\"</td><td>\"+str(class02)+\"</td><td>\"+str(class03)+\"</td><td>\"+str(class04)+\"</td><td>\"+str(round(class00*1.0/(class00+class01+class02+class03+class04),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>\"+\"Class 1\"+\"</td><td>\"+str(class10)+\"</td><td>\"+str(class11)+\"</td><td>\"+str(class12)+\"</td><td>\"+str(class13)+\"</td><td>\"+str(class14)+\"</td><td>\"+str(round(class11*1.0/(class10+class11+class12+class13+class14),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>\"+\"Class 2\"+\"</td><td>\"+str(class20)+\"</td><td>\"+str(class21)+\"</td><td>\"+str(class22)+\"</td><td>\"+str(class23)+\"</td><td>\"+str(class24)+\"</td><td>\"+str(round(class22*1.0/(class20+class21+class22+class23+class24),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>\"+\"Class 3\"+\"</td><td>\"+str(class30)+\"</td><td>\"+str(class31)+\"</td><td>\"+str(class32)+\"</td><td>\"+str(class33)+\"</td><td>\"+str(class34)+\"</td><td>\"+str(round(class33*1.0/(class30+class31+class32+class33+class34),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>\"+\"Class 4\"+\"</td><td>\"+str(class40)+\"</td><td>\"+str(class41)+\"</td><td>\"+str(class42)+\"</td><td>\"+str(class43)+\"</td><td>\"+str(class44)+\"</td><td>\"+str(round(class44*1.0/(class40+class41+class42+class43+class44),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>Overall Accuracy</td><td></td><td></td><td></td><td></td><td></td><td>\"+str(round(accuracyPercentage,3))+\"</td></tr>\"\n",
    "    stringTable+=\"</table>\"\n",
    "    print stringTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy\n",
      "Overall Accuracy is 0.502561496995\n",
      "Class 0: 127659 0 38202 0 76340 242201 0.527078748643\n",
      "Class 1: 0 122921 0 119442 0 242363 0.507177250653\n",
      "Class 2: 76736 0 92613 0 72719 242068 0.382590842243\n",
      "Class 3: 0 88616 0 153489 0 242105 0.633976993453\n",
      "Class 4: 88540 0 41517 0 111631 241688 0.461880606402\n",
      "<table><tr><th>True_Predicted</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th><th>Percentage</th></tr><tr><td>Class 0</td><td>127659</td><td>0</td><td>38202</td><td>0</td><td>76340</td><td>0.527</td></tr><tr><td>Class 1</td><td>0</td><td>122921</td><td>0</td><td>119442</td><td>0</td><td>0.507</td></tr><tr><td>Class 2</td><td>76736</td><td>0</td><td>92613</td><td>0</td><td>72719</td><td>0.383</td></tr><tr><td>Class 3</td><td>0</td><td>88616</td><td>0</td><td>153489</td><td>0</td><td>0.634</td></tr><tr><td>Class 4</td><td>88540</td><td>0</td><td>41517</td><td>0</td><td>111631</td><td>0.462</td></tr><tr><td>Overall Accuracy</td><td></td><td></td><td></td><td></td><td></td><td>0.503</td></tr></table>\n"
     ]
    }
   ],
   "source": [
    "print \"Training Accuracy\"\n",
    "get_accuracy(y_train_pred,y_train)\n",
    "print \"Test Accuracy\"\n",
    "get_accuracy(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table><tr><th>True_Predicted</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th><th>Percentage</th></tr><tr><td>Class 0</td><td>9442428</td><td>4063</td><td>255871</td><td>2899</td><td>169271</td><td>0.956</td></tr><tr><td>Class 1</td><td>60366</td><td>2198705</td><td>2404</td><td>2432131</td><td>0</td><td>0.468</td></tr><tr><td>Class 2</td><td>2706103</td><td>4761</td><td>735012</td><td>1901</td><td>157996</td><td>0.204</td></tr><tr><td>Class 3</td><td>63029</td><td>1526481</td><td>604</td><td>3293692</td><td>0</td><td>0.674</td></tr><tr><td>Class 4</td><td>1947299</td><td>2625</td><td>164360</td><td>1736</td><td>245170</td><td>0.104</td></tr><tr><td>Overall Accuracy</td><td></td><td></td><td></td><td></td><td></td><td>0.626</td></tr></table>\n",
    "\n",
    "<table><tr><th>True_Predicted</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th><th>Percentage</th></tr><tr><td>Class 0</td><td>4041033</td><td>1784</td><td>111051</td><td>1251</td><td>73227</td><td>0.956</td></tr><tr><td>Class 1</td><td>25785</td><td>937891</td><td>1013</td><td>1045475</td><td>0</td><td>0.467</td></tr><tr><td>Class 2</td><td>1164318</td><td>2080</td><td>313568</td><td>857</td><td>67910</td><td>0.202</td></tr><tr><td>Class 3</td><td>27311</td><td>657640</td><td>253</td><td>1409193</td><td>0</td><td>0.673</td></tr><tr><td>Class 4</td><td>834967</td><td>1202</td><td>70521</td><td>785</td><td>104710</td><td>0.103</td></tr><tr><td>Overall Accuracy</td><td></td><td></td><td></td><td></td><td></td><td>0.625</td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy_SpeciesWise(yPred,yTest,species):\n",
    "    if len(yPred)!=len(yTest):\n",
    "        print \"yPred and yTest have different dimensions.\"\n",
    "        return\n",
    "    overallCorrect=0\n",
    "    stringTable=\"<table><tr><th>species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th><th>Percentage</th></tr>\"\n",
    "    #classxy read as true class x predicted class y\n",
    "    class00,class01,class02,class03,class04=0,0,0,0,0\n",
    "    class10,class11,class12,class13,class14=0,0,0,0,0\n",
    "    class20,class21,class22,class23,class24=0,0,0,0,0\n",
    "    class30,class31,class32,class33,class34=0,0,0,0,0\n",
    "    class40,class41,class42,class43,class44=0,0,0,0,0\n",
    "    speciesList=[\"Cow\",\"Cat\",\"Mouse\",\"Pig\",\"Rhesus\",\"Microbat\",\"Elephant\",\"Rabbit\",\"Shrew\"]\n",
    "    enum2 = speciesList.index(species)\n",
    "    #print \"enum2 is\",enum2\n",
    "    for enum1,i in enumerate(yPred):\n",
    "        #print i\n",
    "        j=i[enum2]\n",
    "        j=int(j)\n",
    "        #print j\n",
    "        #print yTest[enum1][enum2]\n",
    "        if j==yTest[enum1][enum2]:\n",
    "            overallCorrect+=1\n",
    "        if yTest[enum1][enum2]==0:\n",
    "            if j==0:\n",
    "                class00+=1\n",
    "            if j==1:\n",
    "                class01+=1\n",
    "            if j==2:\n",
    "                class02+=1\n",
    "            if j==3:\n",
    "                class03+=1\n",
    "            if j==4:\n",
    "                class04+=1\n",
    "        if yTest[enum1][enum2]==1:\n",
    "            if j==0:\n",
    "                class10+=1\n",
    "            if j==1:\n",
    "                class11+=1\n",
    "            if j==2:\n",
    "                class12+=1\n",
    "            if j==3:\n",
    "                class13+=1\n",
    "            if j==4:\n",
    "                class14+=1\n",
    "        if yTest[enum1][enum2]==2:\n",
    "            if j==0:\n",
    "                class20+=1\n",
    "            if j==1:\n",
    "                class21+=1\n",
    "            if j==2:\n",
    "                class22+=1\n",
    "            if j==3:\n",
    "                class23+=1\n",
    "            if j==4:\n",
    "                class24+=1\n",
    "        if yTest[enum1][enum2]==3:\n",
    "            if j==0:\n",
    "                class30+=1\n",
    "            if j==1:\n",
    "                class31+=1\n",
    "            if j==2:\n",
    "                class32+=1\n",
    "            if j==3:\n",
    "                class33+=1\n",
    "            if j==4:\n",
    "                class34+=1\n",
    "        if yTest[enum1][enum2]==4:\n",
    "            if j==0:\n",
    "                class40+=1\n",
    "            if j==1:\n",
    "                class41+=1\n",
    "            if j==2:\n",
    "                class42+=1\n",
    "            if j==3:\n",
    "                class43+=1\n",
    "            if j==4:\n",
    "                class44+=1\n",
    "    accuracyPercentage=overallCorrect*1.0/(len(yPred))\n",
    "    print \"Overall Accuracy is\",accuracyPercentage\n",
    "    print \"Class 0:\",class00,class01,class02,class03,class04,class00+class01+class02+class03+class04,class00*1.0/(class00+class01+class02+class03+class04)\n",
    "    print \"Class 1:\",class10,class11,class12,class13,class14,class10+class11+class12+class13+class14,class11*1.0/(class10+class11+class12+class13+class14)\n",
    "    print \"Class 2:\",class20,class21,class22,class23,class24,class20+class21+class22+class23+class24,class22*1.0/(class20+class21+class22+class23+class24)\n",
    "    print \"Class 3:\",class30,class31,class32,class33,class34,class30+class31+class32+class33+class34,class33*1.0/(class30+class31+class32+class33+class34)\n",
    "    print \"Class 4:\",class40,class41,class42,class43,class44,class40+class41+class42+class43+class44,class44*1.0/(class40+class41+class42+class43+class44)\n",
    "    stringTable+=\"<tr><td>\"+\"Class 0\"+\"</td><td>\"+str(class00)+\"</td><td>\"+str(class01)+\"</td><td>\"+str(class02)+\"</td><td>\"+str(class03)+\"</td><td>\"+str(class04)+\"</td><td>\"+str(round(class00*1.0/(class00+class01+class02+class03+class04),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>\"+\"Class 1\"+\"</td><td>\"+str(class10)+\"</td><td>\"+str(class11)+\"</td><td>\"+str(class12)+\"</td><td>\"+str(class13)+\"</td><td>\"+str(class14)+\"</td><td>\"+str(round(class11*1.0/(class10+class11+class12+class13+class14),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>\"+\"Class 2\"+\"</td><td>\"+str(class20)+\"</td><td>\"+str(class21)+\"</td><td>\"+str(class22)+\"</td><td>\"+str(class23)+\"</td><td>\"+str(class24)+\"</td><td>\"+str(round(class22*1.0/(class20+class21+class22+class23+class24),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>\"+\"Class 3\"+\"</td><td>\"+str(class30)+\"</td><td>\"+str(class31)+\"</td><td>\"+str(class32)+\"</td><td>\"+str(class33)+\"</td><td>\"+str(class34)+\"</td><td>\"+str(round(class33*1.0/(class30+class31+class32+class33+class34),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>\"+\"Class 4\"+\"</td><td>\"+str(class40)+\"</td><td>\"+str(class41)+\"</td><td>\"+str(class42)+\"</td><td>\"+str(class43)+\"</td><td>\"+str(class44)+\"</td><td>\"+str(round(class44*1.0/(class40+class41+class42+class43+class44),3))+\"</td></tr>\"\n",
    "    stringTable+=\"<tr><td>Overall Accuracy</td><td></td><td></td><td></td><td></td><td></td><td>\"+str(round(accuracyPercentage,3))+\"</td></tr>\"\n",
    "    stringTable+=\"</table>\"\n",
    "    print stringTable\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy\n",
      "Overall Accuracy is 0.500458339928\n",
      "Class 0: 293477 24 101983 1 169270 564755 0.519653655125\n",
      "Class 1: 0 293372 75 271146 0 564593 0.519616785897\n",
      "Class 2: 180927 30 225936 0 157995 564888 0.399966010961\n",
      "Class 3: 0 209323 22 355506 0 564851 0.629380137417\n",
      "Class 4: 215248 32 104790 1 245165 565236 0.43373918151\n",
      "<table><tr><th>species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th><th>Percentage</th></tr><tr><td>Class 0</td><td>293477</td><td>24</td><td>101983</td><td>1</td><td>169270</td><td>0.52</td></tr><tr><td>Class 1</td><td>0</td><td>293372</td><td>75</td><td>271146</td><td>0</td><td>0.52</td></tr><tr><td>Class 2</td><td>180927</td><td>30</td><td>225936</td><td>0</td><td>157995</td><td>0.4</td></tr><tr><td>Class 3</td><td>0</td><td>209323</td><td>22</td><td>355506</td><td>0</td><td>0.629</td></tr><tr><td>Class 4</td><td>215248</td><td>32</td><td>104790</td><td>1</td><td>245165</td><td>0.434</td></tr><tr><td>Overall Accuracy</td><td></td><td></td><td></td><td></td><td></td><td>0.5</td></tr></table>\n",
      "Test Accuracy\n",
      "Overall Accuracy is 0.498351405498\n",
      "Class 0: 125289 15 43671 0 73226 242201 0.51729348764\n",
      "Class 1: 0 125179 24 117160 0 242363 0.51649385426\n",
      "Class 2: 77678 8 96473 0 67909 242068 0.398536774791\n",
      "Class 3: 0 90531 6 151568 0 242105 0.626042419611\n",
      "Class 4: 92035 12 44933 0 104708 241688 0.433236238456\n",
      "<table><tr><th>species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th><th>Percentage</th></tr><tr><td>Class 0</td><td>125289</td><td>15</td><td>43671</td><td>0</td><td>73226</td><td>0.517</td></tr><tr><td>Class 1</td><td>0</td><td>125179</td><td>24</td><td>117160</td><td>0</td><td>0.516</td></tr><tr><td>Class 2</td><td>77678</td><td>8</td><td>96473</td><td>0</td><td>67909</td><td>0.399</td></tr><tr><td>Class 3</td><td>0</td><td>90531</td><td>6</td><td>151568</td><td>0</td><td>0.626</td></tr><tr><td>Class 4</td><td>92035</td><td>12</td><td>44933</td><td>0</td><td>104708</td><td>0.433</td></tr><tr><td>Overall Accuracy</td><td></td><td></td><td></td><td></td><td></td><td>0.498</td></tr></table>\n"
     ]
    }
   ],
   "source": [
    "print \"Training Accuracy\"\n",
    "get_accuracy_SpeciesWise(y_train_pred,y_train,\"Cow\")\n",
    "print \"Test Accuracy\"\n",
    "get_accuracy_SpeciesWise(y_pred,y_test,\"Cow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuracy\n",
    "<table><tr><th>species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th><th>Percentage</th></tr><tr><td>Class 0</td><td>293477</td><td>24</td><td>101983</td><td>1</td><td>169270</td><td>0.52</td></tr><tr><td>Class 1</td><td>0</td><td>293372</td><td>75</td><td>271146</td><td>0</td><td>0.52</td></tr><tr><td>Class 2</td><td>180927</td><td>30</td><td>225936</td><td>0</td><td>157995</td><td>0.4</td></tr><tr><td>Class 3</td><td>0</td><td>209323</td><td>22</td><td>355506</td><td>0</td><td>0.629</td></tr><tr><td>Class 4</td><td>215248</td><td>32</td><td>104790</td><td>1</td><td>245165</td><td>0.434</td></tr><tr><td>Overall Accuracy</td><td></td><td></td><td></td><td></td><td></td><td>0.5</td></tr></table>\n",
    "Test Accuracy\n",
    "<table><tr><th>species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th><th>Percentage</th></tr><tr><td>Class 0</td><td>125289</td><td>15</td><td>43671</td><td>0</td><td>73226</td><td>0.517</td></tr><tr><td>Class 1</td><td>0</td><td>125179</td><td>24</td><td>117160</td><td>0</td><td>0.516</td></tr><tr><td>Class 2</td><td>77678</td><td>8</td><td>96473</td><td>0</td><td>67909</td><td>0.399</td></tr><tr><td>Class 3</td><td>0</td><td>90531</td><td>6</td><td>151568</td><td>0</td><td>0.626</td></tr><tr><td>Class 4</td><td>92035</td><td>12</td><td>44933</td><td>0</td><td>104708</td><td>0.433</td></tr><tr><td>Overall Accuracy</td><td></td><td></td><td></td><td></td><td></td><td>0.498</td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del TrainingXOneHotEncoded,TrainingY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unused Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_feature_set(shapelist,NucleoHash):\n",
    "    x=[]\n",
    "    for count,z in enumerate(shapelist):\n",
    "        if(count%1000000==0):\n",
    "            print count\n",
    "        p = []\n",
    "        for j,k in enumerate(z):\n",
    "            if(is_number(k)):\n",
    "                p.append(k)\n",
    "            else:\n",
    "                p.append(NucleoHash[k])\n",
    "        x.append(p)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for enum,i in enumerate(AncestorSequenceList):\n",
    "    indexStr=str(i[0])\n",
    "    firstUnderscore=indexStr.index('_')\n",
    "    intIndex=int(indexStr[0:firstUnderscore])\n",
    "    i[0]=intIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compare the corresponding human index in both lists, a good way to do this is to create a dictionary. We will create a dictionary on combined list. \n",
    "The key of the dictionary will be the human index and the value would we everthing in the combined list except the human index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a={}\n",
    "for i in combinedList:\n",
    "    temp = i[10]\n",
    "    if (not a.has_key(temp)):\n",
    "        a[temp] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G', 'G', 'G', '-', 'G', 'G', '-', 'A', '-', '-', 107, -1, 0, 0, 3, 0, 0, 3, 0, 3, 1]\n",
      "10471798\n"
     ]
    }
   ],
   "source": [
    "print a[107]#check the data format\n",
    "print len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are cases where we have multiple data for same human index, and we don't want this inconsistency in our data so to check on it later, we'll create a dictionary of ancestors so that we can remove data which are multiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b={}\n",
    "for i in AncestorNucleotide:\n",
    "    temp = i[0]\n",
    "    if (not b.has_key(temp)):\n",
    "        b[temp] = i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just remove the duplicates, i will create a list back from the dictionary, this list will not have duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10476466\n"
     ]
    }
   ],
   "source": [
    "AncestorNucleotide=[]\n",
    "for j in b:\n",
    "    AncestorNucleotide.append([j]+[b[j]])\n",
    "print len(AncestorNucleotide)\n",
    "assert(len(b)==len(AncestorNucleotide))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'G', u'G', u'-', u'G', u'G', u'G', u'G', u'G', u'G']\n",
      "[0, [u'G', u'G', u'-', u'G', u'G', u'G', u'G', u'G', u'G']]\n"
     ]
    }
   ],
   "source": [
    "print b[0]\n",
    "print AncestorNucleotide[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create one list which contains the combination of both combined list and ancestor list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalCombinedList = []\n",
    "for i in AncestorNucleotide:\n",
    "    temp = i[0]\n",
    "    if(a.has_key(temp) and b.has_key(temp)):\n",
    "        templist = a[temp]+i[1]\n",
    "        FinalCombinedList.append(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10471798\n",
      "10471798\n",
      "['T', 'T', 'T', '-', 'T', 'T', '-', '-', '-', '-', 2, -1, 0, 0, 3, 0, 0, 3, 3, 3, 1, u'T', u'T', u'-', u'T', u'T', u'T', u'T', u'T', u'T']\n",
      "['T', 'T', 'T', '-', 'T', 'T', '-', '-', '-', '-', 2, -1, 0, 0, 3, 0, 0, 3, 3, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "print len(FinalCombinedList)\n",
    "print len(combinedList)\n",
    "print FinalCombinedList[2]\n",
    "print combinedList[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the order of combinedList and FinalCombinedList is same. So that when we can later combine them for enhancing our feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index,i in enumerate(FinalCombinedList):\n",
    "    if i[10] != combinedList[index][10]:\n",
    "        print \"Warning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have one list we just have to compare the nucleotides at each branch of the phylogeny tree to get the transition/transversion bits. First lets define a fuction which takes 2 nucleotides and outputs the transition/transversion bits. The below fucntion does that. it takes input as 2 nucleotide and returns 6 bits as defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with transition/transversion bits; a good idea would be to also get the sum of transitions;transversions;indels along each column. The below cell gets the bits along with their sum as well. We get 2 lists at the end;\n",
    "1. AncestorTransitionTransversionBits - which contains the bits\n",
    "2. AncestorTransitionTransversionSum - which contains the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 14, 0, 4, 0]\n",
      "['G', 'G', 'G', '-', 'G', 'G', '-', '-', '-', '-', 0, -1, 0, 0, 3, 0, 0, 3, 3, 3, 1, u'G', u'G', u'-', u'G', u'G', u'G', u'G', u'G', u'G']\n",
      "['G', 'G', 'G', '-', 'G', 'G', '-', '-', '-', '-', 0, -1, 0, 0, 3, 0, 0, 3, 3, 3, 1]\n",
      "10471798\n",
      "10471798\n",
      "10471798\n"
     ]
    }
   ],
   "source": [
    "print AncestorTransitionTransversionBits[0]\n",
    "print AncestorTransitionTransversionSum[0]\n",
    "print FinalCombinedList[0]\n",
    "print combinedList[0]\n",
    "print len(combinedList)\n",
    "print len(AncestorTransitionTransversionBits)\n",
    "print len(FinalCombinedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G', 'G', 'G', '-', 'G', 'G', '-', '-', '-', '-', 0, -1, 0, 0, 3, 0, 0, 3, 3, 3, 1, u'G', u'G', u'-', u'G', u'G', u'G', u'G', u'G', u'G', 0, 0, 14, 0, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(FinalCombinedList)):\n",
    "    FinalCombinedList[i] = FinalCombinedList[i] + AncestorTransitionTransversionSum[i]\n",
    "print FinalCombinedList[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the desired ancestor transition transversion feature set. We can delete all the intermediate lists which are no longer required to free up some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each item in the indices list, appending the human index position at the end of each list. This is done so that we can sort the items based on human index because we need to find the flanking region of each index, and it would be easy to process that if the indices are sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for enum,i in enumerate(trainingIndices):\n",
    "    indexStr=str(i[0])\n",
    "    firstUnderscore=indexStr.index('_')\n",
    "    intIndex=int(indexStr[0:firstUnderscore])\n",
    "    i.append(intIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now creating one combined list that would contain the indices and the labels combined so that the labels don't get mapped to the wrong index while sorting. After combining sorting them based on the human index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinedList=[]\n",
    "for index,i in enumerate(range(len(trainingIndices))):\n",
    "    combinedList.append(trainingIndices[index]+trainingLabels[index])\n",
    "combinedList.sort(key=lambda x:x[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have everything in the combined list we can delete the loaded training Indices and training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del trainingIndices,trainingLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'3_Human.chr22_1', u'3_Cow.chr22.1_1', u'3_Cat.chr22.1_1', u'-19', u'3349437_Pig.chr22.1.1+chr22.2.2+chr22.1.1+ch.._1', u'3_Rhesus.chr22_1', u'-19', u'-19', u'-19', u'-19', 3, -1, 0, 0, 3, 0, 0, 3, 3, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "print combinedList[3]#print to see if data is loaded properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now let's use a subset of the combined list; i.e. first million records. So we keep only the first million in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combinedList = combinedList[0:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the range of continous human indices in combined list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the first million combined list items has a continuous human index of 1 to million; if not printing them into ranges of continous numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with storing the human indices of the first million records into a separate list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "for i in combinedList:\n",
    "    data.append(i[10])\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now printing the ranges of the list named \"data\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "ranges = []\n",
    "for key, group in groupby(enumerate(data), lambda (index, item): index - item):\n",
    "    group = map(itemgetter(1), group)\n",
    "    if len(group) > 1:\n",
    "        ranges.append(xrange(group[0], group[-1]))\n",
    "    else:\n",
    "        ranges.append(group[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7063\n",
      "[xrange(7063), xrange(7067, 9397), xrange(9721, 10734), xrange(11766, 17384), xrange(18206, 31722), xrange(31724, 38113), xrange(38129, 38217), xrange(38237, 39363), xrange(39395, 46283), xrange(46312, 53002), xrange(53007, 59781), xrange(59795, 62983), xrange(63039, 64486), xrange(64551, 78885), xrange(78924, 86076), xrange(86078, 86466), xrange(86558, 86767), xrange(86869, 87053), xrange(87455, 88479), xrange(88564, 90074), xrange(90115, 109407), xrange(109409, 109556), xrange(109589, 109822), xrange(109824, 115674), xrange(115686, 117449), xrange(117468, 118801), xrange(118804, 119474), xrange(119488, 122530), xrange(122597, 125730), xrange(125734, 150975), xrange(150996, 159133), xrange(159138, 162568), xrange(162612, 165337), xrange(165339, 173964), xrange(173967, 174233), xrange(176059, 176350), xrange(176355, 178042), xrange(178054, 178055), xrange(178064, 180039), xrange(180104, 180824), xrange(180847, 181491), xrange(181579, 182065), xrange(182069, 182963), xrange(182978, 198340), xrange(198342, 207084), xrange(207537, 218131), xrange(218171, 220079), xrange(220144, 224443), xrange(224446, 233032), xrange(233054, 241380), xrange(241406, 242132), xrange(242142, 251374), xrange(251376, 261713), xrange(261720, 272066), xrange(272707, 287330), xrange(287343, 298628), xrange(298635, 304169), xrange(304176, 304502), xrange(304542, 307774), xrange(307787, 309517), xrange(309531, 309806), xrange(309825, 313986), xrange(314068, 317228), xrange(317231, 323592), xrange(323600, 324444), xrange(324446, 330409), xrange(330415, 330420), xrange(330437, 334351), xrange(334537, 336126), xrange(336142, 338005), xrange(338010, 360279), xrange(360302, 360564), xrange(360567, 361715), xrange(362947, 369223), xrange(369255, 370270), xrange(370272, 379447), xrange(379481, 379737), xrange(379739, 381962), xrange(381970, 385010), xrange(385012, 387087), xrange(387091, 390238), xrange(390642, 391646), xrange(391675, 395292), xrange(395296, 397447), xrange(397450, 399249), xrange(399574, 399787), xrange(399902, 404496), xrange(404690, 405533), xrange(405571, 406028), xrange(406034, 409506), xrange(409921, 411697), xrange(411701, 421686), xrange(421703, 426038), xrange(426055, 427923), xrange(428307, 436590), xrange(436603, 436849), xrange(436857, 445362), xrange(445366, 446609), xrange(446637, 453459), xrange(453478, 461598), xrange(462193, 467424), xrange(467446, 471046), xrange(471061, 471280), xrange(471293, 473557), xrange(473563, 475673), xrange(475694, 478530), xrange(478535, 482601), xrange(482603, 485420), xrange(485425, 495892), xrange(495895, 496901), xrange(496937, 497870), xrange(497873, 498019), xrange(498026, 498660), xrange(498710, 498768), xrange(498817, 501104), xrange(501110, 503451), xrange(503502, 503555), xrange(503583, 513950), xrange(513955, 514530), xrange(514549, 529543), xrange(529555, 531857), xrange(531907, 533329), xrange(533332, 534212), xrange(534222, 534384), xrange(534394, 535417), xrange(535420, 536891), xrange(536911, 537876), xrange(537998, 538013), xrange(538160, 555919), xrange(556039, 560448), xrange(562746, 563092), xrange(563631, 565938), xrange(566154, 566621), xrange(566902, 567463), xrange(567557, 567569), xrange(567807, 574675), xrange(574700, 576942), xrange(576965, 585340), xrange(585349, 589500), xrange(589555, 592411), xrange(592710, 593480), xrange(594661, 624072), xrange(624092, 633298), xrange(633521, 634763), xrange(634771, 656036), xrange(656098, 665950), xrange(666007, 685637), xrange(685773, 693691), xrange(693702, 696857), xrange(697166, 699518), xrange(699631, 699704), xrange(699906, 713712), xrange(713715, 714342), xrange(714373, 721863), xrange(721865, 727919), xrange(728040, 730465), xrange(730556, 732984), xrange(732993, 738020), xrange(738026, 742728), xrange(742747, 742799), xrange(742801, 743080), xrange(743099, 744231), xrange(744235, 744635), xrange(744641, 744672), xrange(744676, 784767), xrange(784841, 791708), xrange(791716, 794487), xrange(794489, 794735), xrange(794744, 795047), xrange(795051, 801725), xrange(801743, 807178), xrange(807186, 807305), xrange(807322, 816513), xrange(816515, 818107), xrange(818242, 822826), xrange(822829, 832410), xrange(832427, 845756), xrange(845790, 847057), xrange(847064, 857475), xrange(857480, 861576), xrange(861620, 872490), xrange(872566, 885583), xrange(885588, 899570), xrange(899573, 899591), xrange(899593, 901965), xrange(901973, 912024), xrange(912026, 920901), xrange(920908, 922142), xrange(922151, 922206), xrange(922209, 922850), xrange(922853, 932452), xrange(932476, 934734), xrange(934744, 936524), xrange(936527, 951759), xrange(951763, 951799), xrange(951821, 952346), xrange(952348, 952639), xrange(952642, 957373), xrange(957380, 958830), xrange(958835, 960561), xrange(960569, 960766), xrange(960777, 962043), xrange(962065, 964637), xrange(964667, 969727), 969758, xrange(969764, 975640), xrange(975642, 977036), xrange(977048, 985017), xrange(985024, 987319), xrange(987323, 990697), xrange(990701, 992746), xrange(992755, 992798), xrange(993010, 1003492), xrange(1003496, 1010829), xrange(1010864, 1012011), xrange(1012013, 1013101), xrange(1013103, 1016873), xrange(1016915, 1018203), xrange(1020135, 1021435)]\n"
     ]
    }
   ],
   "source": [
    "print data[7063]\n",
    "print ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can delete the lists that we don't need anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the nucleotides, we can proceed with enhancing the feature set for our machine learning model. We are currently experimenting with various kind of feature sets. Their description and the corresponding code follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancestor Transition Transversion Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del combinedList,AncestorNucleotide,AncestorTransitionTransversionSum,AncestorTransitionTransversionBits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without flanking regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we only need to separate the combined list and get 2 lists one with indices and the other with labels.\n",
    "The output of this would be 2 indices\n",
    "1. IndicesList- Contains the indices\n",
    "2. LabelList- Contains the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combinedList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-5289fcccd394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mIndicesList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLabelList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombinedList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combinedList' is not defined"
     ]
    }
   ],
   "source": [
    "IndicesList = []\n",
    "LabelList=[]\n",
    "for i in range(len(combinedList)):\n",
    "    if i%10000==0:\n",
    "        print i\n",
    "    IndicesList.append(combinedList[i][0:10])\n",
    "    LabelList.append(combinedList[i][12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(IndicesList[0])\n",
    "print len(LabelList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the required indices and label list; so we can proceed and delete the combined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del combinedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also choose to have flanking regions along with the single column. The below few cells are used to get the flanking regions as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With flanking region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First decide on the number of flanking region on each side and set it below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><th>Species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th></tr><tr><td>Cow</td><td>40.076</td><td>19.257</td><td>9.142</td><td>23.82</td><td>7.706</td><td><tr><td>Cat</td><td>53.187</td><td>14.717</td><td>8.718</td><td>15.56</td><td>7.819</td><td><tr><td>Mouse</td><td>26.723</td><td>24.444</td><td>8.155</td><td>34.087</td><td>6.592</td><td><tr><td>Pig</td><td>50.141</td><td>15.821</td><td>9.012</td><td>17.561</td><td>7.464</td><td><tr><td>Rhesus</td><td>78.394</td><td>5.488</td><td>5.678</td><td>6.136</td><td>4.303</td><td><tr><td>Microbat</td><td>49.576</td><td>16.676</td><td>8.914</td><td>17.777</td><td>7.056</td><td><tr><td>Elephant</td><td>51.389</td><td>15.801</td><td>8.707</td><td>16.754</td><td>7.349</td><td><tr><td>Rabbit</td><td>50.267</td><td>15.603</td><td>9.037</td><td>16.402</td><td>8.69</td><td><tr><td>Shrew</td><td>24.88</td><td>24.961</td><td>8.156</td><td>35.899</td><td>6.105</td><td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desampled Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n"
     ]
    }
   ],
   "source": [
    "IndicesList = []\n",
    "LabelList=[]\n",
    "for i in range(len(DesampedFinal)):\n",
    "    if i%1000000==0:\n",
    "        print i\n",
    "    if i>=noOfNeighborsOnOneSide and i<(len(DesampedFinal)-noOfNeighborsOnOneSide):\n",
    "        tempList=[]\n",
    "        flag =0\n",
    "        for j in reversed(range(1,(noOfNeighborsOnOneSide+1))):\n",
    "            if(DesampedFinal[i-j][10] == DesampedFinal[i-j+1][10]-1):\n",
    "                tempList= tempList + DesampedFinal[i-j][0:10] + DesampedFinal[i-j][21:]\n",
    "            else:\n",
    "                flag = 1\n",
    "        if(flag == 1):\n",
    "            continue\n",
    "        tempList+=DesampedFinal[i][0:10]\n",
    "        for k in range(1,(noOfNeighborsOnOneSide+1)):\n",
    "            if(DesampedFinal[i+k][10] == DesampedFinal[i+k-1][10]+1):\n",
    "                tempList= tempList + DesampedFinal[i+k][0:10]+ DesampedFinal[i+k][21:]\n",
    "            else:\n",
    "                flag =1\n",
    "        if(flag == 1):\n",
    "            continue\n",
    "        LabelList.append(DesampedFinal[i][12:21])\n",
    "        IndicesList.append(tempList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 3, 0, 0, 3, 3, 3, 1]\n",
      "['G', 'G', 'G', '-', 'G', 'G', '-', '-', '-', '-', u'G', u'G', u'-', u'G', u'G', u'G', u'G', u'G', u'G', 0, 0, 14, 0, 4, 0, 'A', 'A', 'A', '-', 'A', 'A', '-', '-', '-', '-', u'A', u'A', u'-', u'A', u'A', u'A', u'A', u'A', u'A', 0, 0, 14, 0, 4, 0, 'T', 'T', 'T', '-', 'T', 'T', '-', '-', '-', '-', u'T', u'T', u'-', u'T', u'T', u'T', u'T', u'T', u'T', 0, 0, 14, 0, 4, 0, 'C', 'C', 'T', '-', 'C', 'C', '-', '-', '-', '-', 'T', 'T', 'T', '-', 'T', 'T', '-', '-', '-', '-', u'T', u'T', u'-', u'T', u'T', u'T', u'T', u'T', u'T', 0, 0, 14, 0, 4, 0, 'G', 'C', 'G', '-', 'G', 'G', '-', 'G', '-', '-', u'G', u'G', u'-', u'G', u'G', u'G', u'G', u'G', u'G', 0, 1, 14, 0, 3, 0, 'A', 'A', 'A', '-', 'A', 'A', '-', 'A', '-', '-', u'A', u'A', u'-', u'A', u'A', u'A', u'A', u'A', u'A', 0, 0, 15, 0, 3, 0]\n",
      "3731533\n",
      "3731533\n"
     ]
    }
   ],
   "source": [
    "print LabelList[0]\n",
    "print IndicesList[0]\n",
    "print len(IndicesList)\n",
    "print len(LabelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table><tr><th>Species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th></tr><tr><td>Cow</td><td>21.425</td><td>20.218</td><td>21.323</td><td>21.217</td><td>15.817</td><td><tr><td>Cat</td><td>43.145</td><td>16.411</td><td>16.359</td><td>15.85</td><td>8.235</td><td><tr><td>Mouse</td><td>20.279</td><td>25.588</td><td>14.242</td><td>33.297</td><td>6.594</td><td><tr><td>Pig</td><td>38.983</td><td>17.621</td><td>17.284</td><td>17.387</td><td>8.725</td><td><tr><td>Rhesus</td><td>70.327</td><td>7.946</td><td>10.927</td><td>5.329</td><td>5.471</td><td><tr><td>Microbat</td><td>39.451</td><td>18.168</td><td>16.538</td><td>18.121</td><td>7.722</td><td><tr><td>Elephant</td><td>41.576</td><td>17.817</td><td>15.755</td><td>17.784</td><td>7.068</td><td><tr><td>Rabbit</td><td>42.562</td><td>16.446</td><td>15.727</td><td>16.589</td><td>8.677</td><td><tr><td>Shrew</td><td>18.15</td><td>27.251</td><td>14.628</td><td>34.363</td><td>5.607</td><td></table>'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_species_wise_class_dist(LabelList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><th>Species</th><th>Class 0</th><th>Class 1</th><th>Class 2</th><th>Class 3</th><th>Class 4</th></tr><tr><td>Cow</td><td>21.425</td><td>20.218</td><td>21.323</td><td>21.217</td><td>15.817</td><td><tr><td>Cat</td><td>43.145</td><td>16.411</td><td>16.359</td><td>15.85</td><td>8.235</td><td><tr><td>Mouse</td><td>20.279</td><td>25.588</td><td>14.242</td><td>33.297</td><td>6.594</td><td><tr><td>Pig</td><td>38.983</td><td>17.621</td><td>17.284</td><td>17.387</td><td>8.725</td><td><tr><td>Rhesus</td><td>70.327</td><td>7.946</td><td>10.927</td><td>5.329</td><td>5.471</td><td><tr><td>Microbat</td><td>39.451</td><td>18.168</td><td>16.538</td><td>18.121</td><td>7.722</td><td><tr><td>Elephant</td><td>41.576</td><td>17.817</td><td>15.755</td><td>17.784</td><td>7.068</td><td><tr><td>Rabbit</td><td>42.562</td><td>16.446</td><td>15.727</td><td>16.589</td><td>8.677</td><td><tr><td>Shrew</td><td>18.15</td><td>27.251</td><td>14.628</td><td>34.363</td><td>5.607</td><td></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del FinalCombinedList,combinedList,DesampedFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Desampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IndicesList = []\n",
    "LabelList=[]\n",
    "for i in range(len(FinalCombinedList)):\n",
    "    if i%1000000==0:\n",
    "        print i\n",
    "    if i>=noOfNeighborsOnOneSide and i<(len(FinalCombinedList)-noOfNeighborsOnOneSide):\n",
    "        tempList=[]\n",
    "        flag =0\n",
    "        for j in reversed(range(1,(noOfNeighborsOnOneSide+1))):\n",
    "            if(FinalCombinedList[i-j][10] == FinalCombinedList[i-j+1][10]-1):\n",
    "                tempList= tempList + FinalCombinedList[i-j][0:10] + FinalCombinedList[i-j][21:]\n",
    "            else:\n",
    "                flag = 1\n",
    "        if(flag == 1):\n",
    "            continue\n",
    "        tempList+=FinalCombinedList[i][0:10]\n",
    "        for k in range(1,(noOfNeighborsOnOneSide+1)):\n",
    "            if(FinalCombinedList[i+k][10] == FinalCombinedList[i+k-1][10]+1):\n",
    "                tempList= tempList + FinalCombinedList[i+k][0:10]+ FinalCombinedList[i+k][21:]\n",
    "            else:\n",
    "                flag =1\n",
    "        if(flag == 1):\n",
    "            continue\n",
    "        LabelList.append(FinalCombinedList[i][12:21])\n",
    "        IndicesList.append(tempList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in FinalCombinedList[0:10]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(LabelList)\n",
    "print len(DesampedFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(len(IndicesList[0])==(((noOfNeighborsOnOneSide*2)+1)*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print IndicesList[0]\n",
    "print FinalCombinedList[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all is good we have our Label and Indices List with us. We can now delete the combinedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del combinedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating X and Y matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what features you want to experiment with you can just add the different lists to get the X_training List. The Y_training List is just the LabelList. So we don't fiddle with that anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_training=IndicesOneHotEncoded\n",
    "#print len(X_training[0])\n",
    "X_training=[]\n",
    "X_training=[a + b for a, b in zip(IndicesOneHotEncoded, AncestorTransitionTransversionBits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102, 102, 102, 104, 102, 102, 104, 104, 104, 104, 102, 102, 104, 102, 102, 102, 102, 102, 102, 0, 0, 14, 0, 4, 0, 100, 100, 100, 104, 100, 100, 104, 104, 104, 104, 100, 100, 104, 100, 100, 100, 100, 100, 100, 0, 0, 14, 0, 4, 0, 103, 103, 103, 104, 103, 103, 104, 104, 104, 104, 103, 103, 104, 103, 103, 103, 103, 103, 103, 0, 0, 14, 0, 4, 0, 101, 101, 103, 104, 101, 101, 104, 104, 104, 104, 103, 103, 103, 104, 103, 103, 104, 104, 104, 104, 103, 103, 104, 103, 103, 103, 103, 103, 103, 0, 0, 14, 0, 4, 0, 102, 101, 102, 104, 102, 102, 104, 102, 104, 104, 102, 102, 104, 102, 102, 102, 102, 102, 102, 0, 1, 14, 0, 3, 0, 100, 100, 100, 104, 100, 100, 104, 100, 104, 104, 100, 100, 104, 100, 100, 100, 100, 100, 100, 0, 0, 15, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "268\n",
      "3731533\n"
     ]
    }
   ],
   "source": [
    "print X_training[0]\n",
    "print len(X_training[0])\n",
    "print len(X_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Y_training=[]\n",
    "#for i in LabelList:\n",
    "#    Y_training.append([i[0]])\n",
    "Y_training = LabelList\n",
    "#X_training = IndicesOneHotEncoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "9\n",
      "[0, 0, 3, 0, 0, 3, 3, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "print len(X_training[0])\n",
    "print len(Y_training[0])\n",
    "print Y_training[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Only run for one output label type prediction (Eg: HC only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [[x] for x in y_pred]\n",
    "y_train_pred = [[x] for x in y_train_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "import json\n",
    "import numpy as np\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"keras_learning_phase:0\", dtype=bool)\n",
      "2521616\n",
      "2521616\n",
      "(?, 9)\n",
      "(?, 160)\n",
      "Tensor(\"embedding_7/Gather:0\", shape=(?, 160, 5), dtype=float32)\n",
      "Tensor(\"flatten_7/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"dense_25/BiasAdd:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"dense_26/BiasAdd:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"dense_27/BiasAdd:0\", shape=(?, 45), dtype=float32)\n",
      "Tensor(\"Reshape_6:0\", shape=(?, 9, 5), dtype=float32)\n",
      "Tensor(\"dense_28/div:0\", shape=(?, 9, 5), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can not squeeze dim[2], expected a dimension of 1, got 5 for 'sparse_softmax_cross_entropy_loss/Squeeze' (op: 'Squeeze') with input shapes: [1,5,5].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-d60aac45c7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#loss = tf.reduce_mean(sparse_categorical_crossentropy(out, preds,weights))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;31m#test_prediction = tf.nn.softmax(preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/losses/losses_impl.pyc\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy\u001b[0;34m(labels, logits, weights, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;31m# therefore, expected_rank_diff=1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     labels, logits, weights = _remove_squeezable_dimensions(\n\u001b[0;32m--> 739\u001b[0;31m         labels, logits, weights, expected_rank_diff=1)\n\u001b[0m\u001b[1;32m    740\u001b[0m     losses = nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n\u001b[1;32m    741\u001b[0m                                                          \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/losses/losses_impl.pyc\u001b[0m in \u001b[0;36m_remove_squeezable_dimensions\u001b[0;34m(labels, predictions, weights, expected_rank_diff)\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0mrank_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_rank\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels_rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrank_diff\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(input, axis, name, squeeze_dims)\u001b[0m\n\u001b[1;32m   2318\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2320\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_squeeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36m_squeeze\u001b[0;34m(input, squeeze_dims, name)\u001b[0m\n\u001b[1;32m   3556\u001b[0m   \"\"\"\n\u001b[1;32m   3557\u001b[0m   result = _op_def_lib.apply_op(\"Squeeze\", input=input,\n\u001b[0;32m-> 3558\u001b[0;31m                                 squeeze_dims=squeeze_dims, name=name)\n\u001b[0m\u001b[1;32m   3559\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can not squeeze dim[2], expected a dimension of 1, got 5 for 'sparse_softmax_cross_entropy_loss/Squeeze' (op: 'Squeeze') with input shapes: [1,5,5]."
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print K.learning_phase()\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "print len(X_train)\n",
    "print len(y_train)\n",
    "InputWidth = 160\n",
    "OutputWidth = 9\n",
    "#print Input[0:2]\n",
    "#print Label[0:2]\n",
    "inp = tf.placeholder(tf.float32, shape=(None,InputWidth))\n",
    "out = tf.placeholder(tf.float32, shape=(None,OutputWidth))\n",
    "print out.get_shape()\n",
    "print inp.get_shape()\n",
    "predictions=[]\n",
    "predictionsTrain =[]\n",
    "EmbeddingDimension = 5\n",
    "OutputWidth = 10\n",
    "learning_rate = 0.001\n",
    "x = Embedding(1000, EmbeddingDimension, input_length=InputWidth)(inp)\n",
    "print x\n",
    "#x = K.reshape(x,(len(X_train)*InputWidth,5))\n",
    "#print x\n",
    "x = Flatten()(x)\n",
    "print x\n",
    "x = Dense(512)(x)\n",
    "print x\n",
    "x = Dense(256)(x)\n",
    "print x\n",
    "#To run for 1 output labels\n",
    "#x = Dense(5)(x)\n",
    "#print x\n",
    "#preds = tf.reshape(x,((tf.shape(inp)[0],1,5)))\n",
    "#print preds\n",
    "#To run for 9 output labels\n",
    "x = Dense(45)(x)\n",
    "print x\n",
    "preds = tf.reshape(x,((tf.shape(inp)[0],9,5)))\n",
    "print preds\n",
    "\n",
    "preds = Dense(5, activation='softmax')(preds)\n",
    "print preds\n",
    "#preds = K.argmax(preds,axis = -1)\n",
    "#print preds\n",
    "#acc_value = accuracy(labels, preds)\n",
    "labels = tf.constant([[1, 2, 3,4,5]])\n",
    "class_weights = tf.constant([[1.0, 2.0, 3.0,4.0,5.0]])\n",
    "weights = tf.gather(class_weights, labels)\n",
    "\n",
    "#loss = tf.reduce_mean(sparse_categorical_crossentropy(out, preds,weights))\n",
    "loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=out, logits=preds,weights =weights))\n",
    "#test_prediction = tf.nn.softmax(preds)\n",
    "p = tf.argmax(preds, axis=2)\n",
    "saver = tf.train.Saver()\n",
    "#prediction=tf.argmax(y,2)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "#acc_value = accuracy(out, preds)\n",
    "with sess.as_default():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        print \"epoch \" + str(epoch)\n",
    "    #train_step.run(feed_dict={inp:X_train,out:y_train,K.learning_phase(): 1})\n",
    "        for i in range(10000):\n",
    "            if(i%1000==0):\n",
    "                print \"iteration number\"+str(i)\n",
    "            train_step.run(feed_dict={inp:X_train[i*250:(i+1)*250],out:y_train[i*250:(i+1)*250],K.learning_phase(): 1})\n",
    "        #temp=acc_value.eval(feed_dict={inp: X_train,out: y_train})\n",
    "        #print type(temp)\n",
    "        #print temp\n",
    "        #save_path = saver.save(sess, \"model_512_5neighbours_ExtraDense.ckpt\")\n",
    "        print out.get_shape()\n",
    "        print preds.get_shape()\n",
    "        #print preds.eval(feed_dict={inp:X_train[0:2],out: y_train[0:2]})\n",
    "        p = tf.argmax(preds, axis=2)\n",
    "        for i in range(10000):\n",
    "            if(i%1000==0):\n",
    "                print \"iteration number\"+str(i)\n",
    "            predictions[i*100:(i+1)*100] = p.eval(feed_dict={inp: X_test[i*100:(i+1)*100],K.learning_phase(): 0})\n",
    "            predictionsTrain[i*250:(i+1)*250] = p.eval(feed_dict={inp: X_train[i*250:(i+1)*250],K.learning_phase(): 0})\n",
    "        print len(predictions)\n",
    "        print len(predictionsTrain)\n",
    "        print \"train accuracy\"\n",
    "        get_accuracy(predictionsTrain,y_train[0:2500000])\n",
    "        print \"test accuracy\"\n",
    "        get_accuracy(predictions,y_test[0:1000000])\n",
    "            \n",
    "    #predictions =  p.eval(feed_dict={inp:X_train[0:2],out: y_train[0:2]})\n",
    "    #print y_train[0:2]\n",
    "    #print predictions\n",
    "    #print precisiondictionOutput\n",
    "    #predictions = sess.run(p,feed_dict={inp: X_test,K.learning_phase(): 0})\n",
    "    #get_accuracy(predictions,y_test[0:300000])\n",
    "\n",
    "    \n",
    "#p = tf.argmax(preds, axis=2)\n",
    "#print p.get_shape()\n",
    "#print p.eval(feed_dict={inp: X_train,out: y_train})\n",
    "'''\n",
    "pout = K.reshape(out,((len(X_test)*9)/9,9))\n",
    "print pout.eval(feed_dict={inp: X_test,out: y_test})\n",
    "print out.eval(feed_dict={inp: X_test,out: y_test})\n",
    "acc_value = accuracy(pout, p)\n",
    "print acc_value\n",
    "#print acc_value.eval()\n",
    "print acc_value.eval(feed_dict={inp: X_test,out: y_test,K.learning_phase(): 0})\n",
    "save_path = saver.save(sess, \"model.ckpt\")\n",
    "'''\n",
    "#Label = np.asarray(Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy\n",
      "Overall Accuracy is 0.5363832\n",
      "Class 0: 387754 0 103094 0 64249 555097 0.698533769774\n",
      "Class 1: 0 217681 0 305325 0 523006 0.416211286295\n",
      "Class 2: 171012 0 235074 0 63471 469557 0.500629316569\n",
      "Class 3: 0 146828 0 402552 0 549380 0.732738723652\n",
      "Class 4: 210709 0 94354 0 97897 402960 0.242944709152\n",
      "Test Accuracy\n",
      "Overall Accuracy is 0.536298\n",
      "Class 0: 155150 0 41375 0 25336 221861 0.699311731219\n",
      "Class 1: 0 87131 0 122608 0 209739 0.415425838781\n",
      "Class 2: 68467 0 93983 0 25524 187974 0.499978720461\n",
      "Class 3: 0 58753 0 161068 0 219821 0.732723443165\n",
      "Class 4: 84018 0 37621 0 38966 160605 0.242620092774\n"
     ]
    }
   ],
   "source": [
    "print \"Training Accuracy\"\n",
    "get_accuracy_SpeciesWise(predictionsTrain,y_train[0:2500000],\"Cow\")\n",
    "print \"Test Accuracy\"\n",
    "get_accuracy_SpeciesWise(predictions,y_test[0:1000000],\"Cow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN Architecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "print K.learning_phase()\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Bidirectional\n",
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "#Input = Input[0:2]\n",
    "#Label = Label[0:2]\n",
    "#Input =[]\n",
    "#Label=[]\n",
    "#Input = [[[1],[2],[2],[4],[5],[4],[3],[4],[5],[5]],[[1],[2],[2],[2],[5],[3],[3],[4],[5],[5]]]\n",
    "#Label = [[[1],[0],[1],[2],[1],[0],[4],[4],[2],[1]],[[2],[0],[1],[2],[1],[0],[4],[4],[2],[1]]]\n",
    "#Input = [[1,2,3,4,5,4,5,3,2,1],[1,2,3,4,5,4,3,2,1,2]]\n",
    "#Label = [[1,2,3,4,0,4,0,3,2,1],[1,2,3,4,0,4,3,2,1,2]]\n",
    "#TInput = [[1,2,3,4,5,4,5,3,2,1],[1,2,3,4,5,4,3,2,1,2]]\n",
    "#TLabel = [[1,2,3,4,0,4,0,3,2,1],[1,2,3,4,0,4,3,2,1,2]]\n",
    "'''\n",
    "Input = [[[1,0,0,0,0],[0,1,0,0,0],[0,1,0,0,0],[0,0,0,1,0],[0,0,0,0,1],[0,0,0,1,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1],\n",
    "          [0,0,0,0,1]],\n",
    "         [[1,0,0,0,0],[0,1,0,0,0],[0,1,0,0,0],[0,1,0,0,0],[0,0,0,0,1],[0,0,1,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1],\n",
    "          [0,0,0,0,1]]]\n",
    "\n",
    "Label = [[[1,0,0,0,0],[1,0,0,0,0],[0,1,0,0,0],[0,1,0,0,0],[0,0,0,1,0],[0,0,0,0,1],[0,0,0,1,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1]],\n",
    "         [[1,0,0,0,0],[1,0,0,0,0],[0,1,0,0,0],[0,1,0,0,0],[0,1,0,0,0],[0,0,0,0,1],[0,0,1,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1]]\n",
    "         ]\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "#print Input[0:2]\n",
    "#print Label[0:2]\n",
    "inp = tf.placeholder(tf.float32, shape=(None,250))\n",
    "out = tf.placeholder(tf.float32, shape=(None,9))\n",
    "print out.get_shape()\n",
    "print inp.get_shape()\n",
    "t = inp.get_shape().as_list()\n",
    "print t[0]\n",
    "EmbeddingDimension = 6\n",
    "InputWidth = 250\n",
    "learning_rate=0.001\n",
    "predictions =[]\n",
    "predictionsTrain =[]\n",
    "x = Embedding(10, EmbeddingDimension, input_length=InputWidth)(inp)\n",
    "print x\n",
    "x = Flatten()(x)\n",
    "print x\n",
    "x = Dense(512)(x)\n",
    "print x\n",
    "x = Dense(256)(x)\n",
    "print x\n",
    "#shape = K.shape(x)\n",
    "#pool_shape = tf.stack([t[0]*InputWidth,200])  # Here you can mix integers and symbolic elements of `shape`\n",
    "#x = tf.reshape(x, pool_shape)\n",
    "#x = Dropout(0.5)(x)\n",
    "#print x\n",
    "x = Dense(45)(x)\n",
    "print x\n",
    "#preds = x\n",
    "preds = tf.reshape(x,((tf.shape(inp)[0],9,5)))\n",
    "print preds\n",
    "preds = Dense(5,activation='softmax')(preds)\n",
    "print preds\n",
    "#preds = K.argmax(preds,axis = -1)\n",
    "#print preds\n",
    "\n",
    "loss = tf.reduce_mean(sparse_categorical_crossentropy(out, preds))\n",
    "#test_prediction = tf.nn.softmax(preds)\n",
    "p = tf.argmax(preds, axis=2)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "    #train_step.run(feed_dict={inp:X_train,out:y_train,K.learning_phase(): 1})\n",
    "        for i in range(10000):\n",
    "            if(i%1000==0):\n",
    "                print \"iteration number\"+str(i)\n",
    "            train_step.run(feed_dict={inp:X_train[i*500:(i+1)*500],out:y_train[i*500:(i+1)*500],K.learning_phase(): 1})\n",
    "        #temp=acc_value.eval(feed_dict={inp: X_train,out: y_train})\n",
    "        #print type(temp)\n",
    "        #print temp\n",
    "        #save_path = saver.save(sess, \"model_512_5neighbours_ExtraDense.ckpt\")\n",
    "        print out.get_shape()\n",
    "        print preds.get_shape()\n",
    "        #print preds.eval(feed_dict={inp:X_train[0:2],out: y_train[0:2]})\n",
    "        p = tf.argmax(preds, axis=2)\n",
    "        for i in range(1000):\n",
    "            if(i%100==0):\n",
    "                print \"iteration number\"+str(i)\n",
    "            predictions[i*300:(i+1)*300] = p.eval(feed_dict={inp: X_test[i*300:(i+1)*300],K.learning_phase(): 0})\n",
    "            predictionsTrain[i*300:(i+1)*300] = p.eval(feed_dict={inp: X_train[i*300:(i+1)*300],K.learning_phase(): 0})\n",
    "    #predictions =  p.eval(feed_dict={inp:X_train[0:2],out: y_train[0:2]})\n",
    "    #print y_train[0:2]\n",
    "    #print predictions\n",
    "    #print precisiondictionOutput\n",
    "    #predictions = sess.run(p,feed_dict={inp: X_test,K.learning_phase(): 0})\n",
    "    #get_accuracy(predictions,y_test[0:300000])\n",
    "#Label = np.asarray(Label)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
